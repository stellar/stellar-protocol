```
CAP: 0066
Title: Soroban In-memory Read Resource
Working Group:
    Owner: Garand Tyson <@SirTyson>
    Authors: Garand Tyson <@SirTyson>
    Consulted: Dmytro Kozhevin <@dmkozh>, Nicolas Barry <@MonsieurNicolas>
Status: Draft
Created: 2024-12-09
Discussion: https://github.com/stellar/stellar-protocol/discussions/1585
Protocol version: 23
```

## Simple Summary

This proposal introduces a new resource type for Soroban reads, distinguishing between in-memory and
disk reads. This also proposes automatic restoration for archived entries via `InvokeHostFunctionOp`.

## Working Group

As specified in the Preamble.

## Motivation

By distinguishing disk and in-memory reads, this proposal allows for significant increase in Soroban
read limits. This distinction also allows for safe automatic entry restoration, significantly improving UX.

### Goals Alignment

This change is aligned with the goal of lowering the cost and increasing the scale of the network.

## Abstract

[CAP-0062](cap-0062.md) introduces partial State Archival, where evicted and live Soroban state is stored in separate databases.
This separation allows live Soroban state to be efficiently cached entirely in memory, removing disc reads entirely for all
live Soroban state. Todays eviction scan can then delete entries from both the live BucketList disk and in-memory cache efficiently.
However, classic state and evicted entries are still subject to disk reads.

This CAP introduces a new resource type for reads, distinguishing between disk reads and in-memory reads. By making this
distinction at the protocol level, read limits for Soroban data can greatly increase. This also opens the door for other
optimizations, such as a complete module cache for all live contracts.

Additionally, this CAP introduces automatic restoration via `InvokeHostFunctionOp`, where any archived key present in the
footprint is automatically restored. Initially in protocol 20, the state archival design was not solidified enough to
enable automatic restoration, so an explicit restore operation was required. While this was not technically required in
protocol 20, it was important for contract interfaces to reason properly about restoration such that when full state
archival was introduce, it would not break preexisting deployments. Given that the full State Archival design has been
mostly finalized in [CAP-0057](cap-0057.md), it is now appropriate to introduce automatic restoration.

## Specification

### XDR changes

```diff mddiffcheck.ignore=true
--- a/Stellar-contract-config-setting.x
+++ b/Stellar-contract-config-setting.x
@@ -26,38 +26,48 @@ struct ConfigSettingContractComputeV0
 // Ledger access settings for contracts.
 struct ConfigSettingContractLedgerCostV0
 {
-    // Maximum number of ledger entry read operations per ledger
-    uint32 ledgerMaxReadLedgerEntries;
-    // Maximum number of bytes that can be read per ledger
-    uint32 ledgerMaxReadBytes;
+    // Maximum number of disk entry read operations per ledger
+    uint32 ledgerMaxDiskReadEntries;
+    // Maximum number of bytes of disk reads that can be performed per ledger
+    uint32 ledgerMaxDiskReadBytes;
     // Maximum number of ledger entry write operations per ledger
     uint32 ledgerMaxWriteLedgerEntries;
     // Maximum number of bytes that can be written per ledger
     uint32 ledgerMaxWriteBytes;
 
-    // Maximum number of ledger entry read operations per transaction
-    uint32 txMaxReadLedgerEntries;
-    // Maximum number of bytes that can be read per transaction
-    uint32 txMaxReadBytes;
+    // Maximum number of disk entry read operations per transaction
+    uint32 txMaxDiskReadEntries;
+    // Maximum number of bytes of disk reads that can be performed per transaction
+    uint32 txMaxDiskReadBytes;
     // Maximum number of ledger entry write operations per transaction
     uint32 txMaxWriteLedgerEntries;
     // Maximum number of bytes that can be written per transaction
     uint32 txMaxWriteBytes;
 
-    int64 feeReadLedgerEntry;  // Fee per ledger entry read
-    int64 feeWriteLedgerEntry; // Fee per ledger entry write
+    int64 feeDiskReadLedgerEntry;  // Fee per disk ledger entry read
+    int64 feeDiskRead1KB;          // Fee for reading 1KB disk
+    int64 feeWriteLedgerEntry;     // Fee per ledger entry write
 
-    int64 feeRead1KB;  // Fee for reading 1KB
 
     // The following parameters determine the write fee per 1KB.
-    // Write fee grows linearly until bucket list reaches this size
-    int64 bucketListTargetSizeBytes;
-    // Fee per 1KB write when the bucket list is empty
-    int64 writeFee1KBBucketListLow;
-    // Fee per 1KB write when the bucket list has reached `bucketListTargetSizeBytes` 
-    int64 writeFee1KBBucketListHigh;
-    // Write fee multiplier for any additional data past the first `bucketListTargetSizeBytes`
-    uint32 bucketListWriteFeeGrowthFactor;
+    // Write fee grows linearly until soroban state reaches this size
+    int64 sorobanStateTargetSizeBytes;
+    // Fee per 1KB write when the soroban state is empty
+    int64 writeFee1KBSorobanStateLow;
+    // Fee per 1KB write when the soroban state has reached `sorobanStateTargetSizeBytes`
+    int64 writeFee1KBSorobanStateHigh;
+    // Write fee multiplier for any additional data past the first `sorobanStateTargetSizeBytes`
+    uint32 sorobanStateWriteFeeGrowthFactor;
+};
+
+// Ledger access settings for contracts.
+struct ConfigSettingContractLedgerCostExtV0
+{
+    // Maximum number of in-memory ledger entry read operations per transaction
+    uint32 txMaxInMemoryReadEntries;
+    // Fee per 1 KB  write of 'classic' entries
+    uint32 feeClassicWrite1KB;
 };
 
 // Historical data (pushed to core archives) settings for contracts.
@@ -302,7 +312,8 @@ enum ConfigSettingID
     CONFIG_SETTING_STATE_ARCHIVAL = 10,
     CONFIG_SETTING_CONTRACT_EXECUTION_LANES = 11,
     CONFIG_SETTING_BUCKETLIST_SIZE_WINDOW = 12,
-    CONFIG_SETTING_EVICTION_ITERATOR = 13
+    CONFIG_SETTING_EVICTION_ITERATOR = 13,
+    CONFIG_SETTING_CONTRACT_LEDGER_COST_EXT_V0 = 14
 };
 
 union ConfigSettingEntry switch (ConfigSettingID configSettingID)
@@ -335,5 +346,7 @@ case CONFIG_SETTING_BUCKETLIST_SIZE_WINDOW:
     uint64 bucketListSizeWindow<>;
 case CONFIG_SETTING_EVICTION_ITERATOR:
     EvictionIterator evictionIterator;
+case CONFIG_SETTING_CONTRACT_LEDGER_COST_EXT_V0:
+    ConfigSettingContractLedgerCostExtV0 contractLedgerCostExt;
 };
 }
diff --git a/Stellar-ledger.x b/Stellar-ledger.x
--- a/Stellar-ledger.x
+++ b/Stellar-ledger.x
@@ -528,12 +528,11 @@ struct LedgerCloseMetaV1
     // systems calculating storage fees correctly.
     uint64 totalByteSizeOfBucketList;
 
-    // Temp keys that are being evicted at this ledger.
-    LedgerKey evictedTemporaryLedgerKeys<>;
+    // TTL and data/code keys that have been evicted at this ledger.
+    LedgerKey evictedKeys<>;
 
-    // Archived restorable ledger entries that are being
-    // evicted at this ledger.
-    LedgerEntry evictedPersistentLedgerEntries<>;
+    // Maintained for backwards compatibility, should never be populated.
+    LedgerEntry unused<>;
 };
 
 union LedgerCloseMeta switch (int v)
diff --git a/Stellar-transaction.x b/Stellar-transaction.x
index 7d32481..f966640 100644
--- a/Stellar-transaction.x
+++ b/Stellar-transaction.x
@@ -882,16 +882,30 @@ struct SorobanResources
     // The maximum number of instructions this transaction can use
     uint32 instructions; 
 
-    // The maximum number of bytes this transaction can read from ledger
-    uint32 readBytes;
+    // The maximum number of bytes this transaction can read from disk backed entries
+    uint32 diskReadBytes;
     // The maximum number of bytes this transaction can write to ledger
     uint32 writeBytes;
 };
 
+struct SorobanResourcesExtV0
+{
+    // Vector of indices representing what Soroban
+    // entries in the footprint are archived, based on the
+    // order of keys provided in the readWrite footprint.
+    uint32 archivedSorobanEntries<>;
+};
+
 // The transaction extension for Soroban.
 struct SorobanTransactionData
 {
-    ExtensionPoint ext;
+    union switch (int v)
+    {
+    case 0:
+        void;
+    case 1:
+        SorobanResourcesExtV0 resourceExt;
+    } ext;
     SorobanResources resources;
     // Amount of the transaction `fee` allocated to the Soroban resource fees.
     // The fraction of `resourceFee` corresponding to `resources` specified 
```

### Semantics

#### In-memory vs Disk Read Resources

Read resources are now separated between memory backed and disk backed state as follows:

- in-memory entries:
  - Live Soroban entries
- disk entries:
  - Archived Soroban entries
  - Classic entries

In-memory entries are subject to the corresponding in-memory limits, but charge no entry or byte based
read fees. On disk entries are subject to the corresponding limits and also charge entry and byte based
read fees.

#### Write fee computation changes

The write fee for the Soroban ledger entries is still computed using the logic defined by [CAP-46-07](cap-0046-07.md#ledger-data). However, instead of using the size of the whole bucket list, only the size of the Soroban state will be used. The name changes in the configuration XDR reflect this semantic change. The write fee will still be used to compute the total fee for writing the Soroban entries, as well as the rent payments.

Since the write fee is based on the Soroban state now, it does not make sense to charge the same fee for writing the classic entries. Moreover, protocol currently doesn't provide any way to increase the size of the classic state as there is no way to create new classic entries from the Soroban operations. This is why
the flat per-KB write fee is introduced for the classic state: `feeClassicWrite1KB`.

Since classic entries accessible to Soroban (account and trustline entries) have relatively stable and predictable size, there is no need to provide a separate value for the write bytes for classic writes. Instead, the following computations are performed to compute the total non-refundable write KB fee:

- Define `CLASSIC_ENTRY_SIZE = 128` constant
- Define `classic_write_count` as the number of classic entries in the `readWrite` footprint of the transaction
- `classic_write_size = CLASSIC_ENTRY_SIZE * classic_write_count`
- `soroban_write_size = tx.sorobanData.resources.writeBytes - classic_write_size` (if this value is negative, transaction is not valid) 
- `write_fee = ceil(classic_write_size * feeClassicWrite1KB / 1024) + ceil(soroban_write_size * feeSoroban)`

Since the fee is now based on `CLASSIC_ENTRY_SIZE`, this has to be reflected in the limit enforcement logic as well. Specifically, when computing the total size of the entries written by the transaction, the size of modified classic entries is assumed to be `CLASSIC_ENTRY_SIZE` instead of their actual XDR size.

#### Initial Network Config Settings

Initially, all disk read limits and fees should match exactly the limits and fees of reads today. Limits must not
decrease from current values in order to prevent bricking existing contracts. In the worst case, a contract can read
up to `txMaxReadEntries - 1` classic entries today (all classic entries + SAC instance). Current limits have been
carefully measured assuming disk access only, so the addition of in-memory state should not negatively affect the network
in any way should we maintain these values via disk read limits.

In-memory read limits are not required wrt execution time, as they are very inexpensive. For this reason, no ledger wide read limit is necessary,
and no byte based read limit is necessary for them. However, the size of TX footprints
does impact the cost and complexity of assembling transactions sets and potentially maintaining the mempool, as with implementation [CAP-63](./CAP-63) Core will need to verify the presence of the conflicts in transaction footprints. Thus a tx entry limit is introduced to ensure efficient transaction set
construction. 

Note, that there is also a 'soft' limit on the maximum number of reads per transaction that is caused by the transaction size limit. But coupling the limits in this fashion is risky - for example, network might need to increase the minimal transaction size in order to accommodate larger contracts, but the simultaneous increase in in-memory entry count might not be desired.

When the protocol is upgraded to version 23 the initial values of the new configuration settings will be set to the following values:
- `txMaxInMemoryReadEntries` will be set to the current value of `txMaxDiskReadEntries` (renamed from `txMaxReadLedgerEntries`) in order to prevent breakages of any existing contracts
- `feeClassicWrite1KB` will be set to `10000` stroops, which roughly matches the current Soroban write fee per 1 KB

#### `archivedEntries` Vector

For the purposes of block creation, the `LedgerKey` alone is sufficient to distinguish between classic and Soroban data.
However, a disk read would be required prior to tx application in order to determine if a Soroban entry was live or archived.
This is necessary for determining what resources limits a Soroban key counts towards, but
exposes a significant DOS angle. To prevent this attack, the footprint must statically declare which entries are live vs. archived.

This is accomplished via the `archivedEntries` vector. If any archived Soroban entry is present in a TX's footprint,
it must provide the `archivedEntries` vector containing the index of each archived key based on the ordering of the
readWrite footprint. Because restores are a write operation, no archived key should be in the readOnly footprint. This vector
should contain no classic key indexes, as classic keys are always considered disk reads.

#### Changes to `InvokeHostFunctionOp`

##### Automatic Entry Restoration

Whenever `InvokeHostFunctionOp` is applied, any archived state is
automatically restored prior to host function invocation. This restored state is then accessible to the host function.

Restored state is still subject to the same minimum rent and write fees that exist currently. All entries being restored
count towards disk read resources and fees. Even if an entry is archived but not yet evicted such that it technically still
exists in memory, it is still subject to the same limits and fees as disk based entries in order ot provide a simpler unified
interface for downstream systems.

All archived keys must be declared in the `archivedEntries` vector via `SorobanResourcesExtV0`.
If no archived entries are present in the footprint, `SorobanResourcesExtV0` may be omitted.

##### Incorrect Entries in `archivedEntries` Vector

If a key is not marked as archived, but the entry is actually archived, then the TX fails. If a Soroban key is marked as archived when
it is not, the transaction does not fail. The entry is treated as if it is archived from a resource and fee perspective.
This is appropriate, as on disk reads are more expensive than in-memory. So long as the TX pays the disk based fees,
there is no issue with actually loading an in-memory entry.

If a classic key is marked as archived, the TX fails, as the footprint is malformed.

##### Failure Codes

If an archived entry is included in the TX's footprint but is not specified via the `archivedEntries` vector, the
tx will fail with the `INVOKE_HOST_FUNCTION_ENTRY_ARCHIVED` code. If there are insufficient resources for entry restoration,
the tx will fail with the `INVOKE_HOST_FUNCTION_RESOURCE_LIMIT_EXCEEDED` code.

If a classic entry is marked as archived, the TX fails with `INVOKE_HOST_FUNCTION_MALFORMED` during apply time.

##### Meta

Whenever an entry is restored, the data/code entry will be emitted as a `LedgerEntryChangeType` of type `LEDGER_ENTRY_RESTORE`.
This will include the complete LedgerEntry of the restored entry. It does not matter if the entry has been
evicted or not, the meta produced will be the same. Additionally, the corresponding `TTL` entry will also be emitted
as a `LedgerEntryChangeType` of type `LEDGER_ENTRY_RESTORE`.

If a restored entry is modified during host function invocation, another `LedgerEntryChangeType` will be
emitted corresponding to the entry change in addition to the `LEDGER_ENTRY_RESTORE` restore event.
This is similar to the current meta schema, where the starting value of a LedgerEntry is emitted as
`LEDGER_ENTRY_STATE` followed by the change. This is similar, except that `LEDGER_ENTRY_STATE` is replaced
by `LEDGER_ENTRY_RESTORE`. This applies to both data/code and `TTL` entries. For example, suppose a data/code entry
is restored via an `InvokeHostFunctionOp`, then during invocation its `TTL` value is bumped. Meta for the TX will be
as follows:

```
data/code: LEDGER_ENTRY_RESTORE(restoredValue)
TTL: LEDGER_ENTRY_RESTORE(minimumTTLfromRestore), LEDGER_ENTRY_UPDATED(bumpedTTLValue)
```

Similarly, suppose a `CONTRACT_DATA` entry is restored, then deleted during invocation. Meta for the TX will be:

```
CONTRACT_DATA: LEDGER_ENTRY_RESTORE(restoredValue), LEDGER_ENTRY_REMOVED(dataKey)
TTL: LEDGER_ENTRY_RESTORE(minimumTTLfromRestore), LEDGER_ENTRY_REMOVED(ttlKey)
```

If an entry is restored and not modified (either from RestoreFootprintOp or InvokeHostFunctionOp), the meta will be:

```
CONTRACT_DATA: LEDGER_ENTRY_RESTORE(restoredValue)
TTL: LEDGER_ENTRY_RESTORE(minimumTTLfromRestore)
```

#### getledgerentry Endpoint

In order to facilitate state queries for RPC preflight, captive-core will expose the following HTTP endpoint:

`getledgerentry`

Used to query both live and archived LedgerEntries. While `getledgerentryraw` does simple key-value lookup
on the live ledger, `getledgerentry` will query a given key in both the live BucketList and Hot Archive BucketList.
It will also report whether an entry is archived or live, and return the entry's current TTL value.

A POST request with the following body:

```http
ledgerSeq=NUM&key=Base64&key=Base64...
```

- `ledgerSeq`: An optional parameter, specifying the ledger snapshot to base the query on.
  If the specified ledger in not available, a 404 error will be returned. If this parameter
  is not set, the current ledger is used.
- `key`: A series of Base64 encoded XDR strings specifying the `LedgerKey` to query. TTL keys
  must not be queried and will return 400 if done so.

A JSON payload is returned as follows:

```json
{
"entries": [
     {"e": "Base64-LedgerEntry", "state": "live", /*optional*/ "ttl": uint32},
     {"e": "Base64-LedgerKey", "state": "new"},
     {"e": "Base64-LedgerEntry", "state": "archived"},
],
"ledger": ledgerSeq
}
```

- `entries`: A list of entries for each queried LedgerKey. Every `key` queried is guaranteed to
have a corresponding entry returned.
- `e`: Either the `LedgerEntry` or `LedgerKey` for a given key encoded as a Base64 string. If a key
is live or archived, `e` contains the corresponding `LedgerEntry`. If a key does not exist
(including expired temporary entries) `e` contains the corresponding `LedgerKey`.
- `state`: One of the following values:
  - `live`: Entry is live.
  - `new`: Entry does not exist. Either the entry has never existed or is an expired temp entry.
  - `archived`: Entry is archived, counts towards on-disk resources.
- `ttl`: An optional value, only returned for live Soroban entries. Contains
a uint32 value for the entry's `liveUntilLedgerSeq`.
- `ledgerSeq`: The ledger number on which the query was performed.

Classic entries will always return a state of `live` or `new`.
If a classic entry does not exist, it will have a state of `new`.

Similarly, temporary Soroban entries will always return a state of `live` or
`new`. If a temporary entry does not exist or has expired, it
will have a state of `new`.

This endpoint will always give correct information for archived entries. Even
if an entry has been archived and evicted to the Hot Archive, this endpoint will
still the archived entry's full `LedgerEntry` as well as the proper state.

## Design Rationale

### Reasoning behind in-memory limits and no fees

While there is a resource limit for the maximum number of in-memory entry reads, there is no limit on the
number of in-memory read bytes. Additionally, there are no read-specific fees for in-memory state.

There does not seem to be a need for limits on in-memory bytes read. The cost associated with loading an
in-memory entry is searching an in-memory map and making a copy of the LedgerEntry. These copies will be
metered against the `txMemoryLimit` limit. To prevent OOM based DOS, operation apply will exit early if
loading entries exceeds this limit.

There is very little execution time associated with copying an in-memory entry. Given that the size of the
copies is bounded by the `txMemoryLimit` and no expensive disk IO is required, it is not necessary to charge
a fee for in-memory reads. An attacker could potentially exploit this
by creating TXs with large readOnly footprints that don't actually interact with the loaded state. The
TX would consume little resources and would be cheap, but would load the maximum amount of in-memory state
and. Due to the low computational cost of these in-memory copies, it seems unlikely this would negatively
affect the network. Even the TX size costs associated with the large footprint may make the attack economically
nonviable relative to the amount of stress on the network. Additionally, memory allocations are short lived and
freed immediately following TX application. Only a small number of TXs are executed at any given time such that
memory is quickly freed such that OOM based attacks are not possible.

### UX Expectations

With the addition of the new read resource type, contract developers will need to reason about what types of data
they are accessing wrt limits. However, the system has been designed such that this should be straight forward.
From a correctness standpoint, all smart contracts can be guaranteed to only interact with live state. A contract
developer can assume that all Soroban state consumes only in-memory resources, since the `RestoreFootprintOp`
(or equivalent in the case of deprecation) can restore all required Soroban entries before host function invocation
in the worst case. This means that from a limits perspective, only Soroban state vs. classic state must be considered
for contract correctness. However, there may be issues with usability or efficiency, discussed in the section below.

While contract developers only need to distinguish between Soroban and classic state for contract correctness, the
tx itself will still need to properly populate limits and the `archivedEntries` vector based on whether or not
entries are archived. Similar to today, these resources and footprints will be generated automatically via RPC.
Specifically, RPC will correctly generate the footprint, `archivedEntries` vector, in-memory read resources,
and disk read resources. Thus most of the complexity introduced by these changes should be abstracted away.

Note that the RPC itself is not required to implement this behavior, but will call captive-core endpoints.

### Expectations for Downstream Systems

RPC must make the following changes:

1. Ingest the new `LEDGER_ENTRY_RESTORE` restore changes. This includes interpreting the new `LedgerEntryChangeType`
correctly, but also correctly handling the case where an entry is restored and modified in the same ledger.
2. Allow user queries for both live and evicted state via the `getledgerentry` endpoint.
3. Handle transactions that cannot use automatic restore due to resource limits.

Change 1 should be straight forward and is defined in the [InvokeHostFunctionOp Meta](#meta) section.

Change 2 allows wallets and dapps to effectively reason about ledger state. RPC integration is minimal, as internally
this endpoint can be implemented by calling the captive-core [`getledgerentry` HTTP endpoint](#getledgerentry-endpoint).

Change 3 is more involved and discussed in the section below.

### Handling Invocations that restore too much state for automatic restore

Suppose an expensive contract is written, such that it uses the maximum in-memory resources and the maximum
disk resources when no entries are archived. Should any Soroban entry be archived automatic
restoration would not be possible, as the restoration would exceed resource limits. In this case, it would be
necessary to manually issue one or more`RestoreFootprintOp` prior to the
host function invocation.

To facilitate this, RPC must change the structure of `restorePreamble` to allow for multiple `RestoreFootprintOp`s.
Instead of returning a single set of keys and resource object in `restorePreamble`, it should return a vector of
{keySet, resourceObject} pairs, where each pair represents a `RestoreFootprintOp`. This vector should be constructed
as follows:

```python
restorePreamble = []
archivedKeys = keysReturnedFromSimulationLibrary
invokeHostFunctionOp = preflightedOp

# First, append as many archived keys as possible to be auto restored via invokeHostFunctionOp
while invokeHostFunctionOp.hasSpaceInDiskResources() and not archivedKeys.empty():
     keyToAutoRestore = archivedKeys.pop()

     # Function will mark the key as archived in the footprint and add
     # the required diskRead resources to invokeHostFunctionOp
     markAsArchivedEntry(invokeHostFunctionOp, keyToAutoRestore)

# If we get here, there are more archived keys than can be restored via invokeHostFunctionOp auto restore
# We need to construct RestoreOps to restore the remaining keys
if not archivedKeys.empty():
     restorePreamble.append(RestoreOp())
     while not archivedKeys.empty():
          while restorePreamble[-1].hasSpaceInDiskResources() and not archivedKeys.empty():
               keyToRestore = archivedKeys.pop()
               restorePreamble[-1].addKey(keyToRestore)

          # If we filled up out first restore op, create a new one
          if not archivedKeys.empty():
               restorePreamble.append(RestoreOp())
```

Ideally, developers should be avoid creating contracts that require these expensive restores if possible.
The challenge will be communicating this to developers, but may be accomplished via documentation.

### Concerns regarding Automatic Restoration with Full State Archival

In protocol 20, there was no technical reason against enabling automatic restoration. However, the proof system
for full state archival was not finalized, and the explicit `RestoreFootprintOp` provided more future flexibility.
Additionally, it was important for developers to build preflighting transactions into their dApp flows.

If Soroban had launched with automatic restoration, it is possible many developers would attempt to avoid preflight
by creating transaction footprint and resource declarations manually with reasonable fee and resource assumptions.
In a system where explicit entry restoration is not required, the burden of resource discoverability is low, so
developers are not incentivized or required to build preflight into their tx submission system. Long term this is
a significant issue, as the addition of full state archival with proofs makes manual resource and footprint
declaration largely impossible. Any system that did not use RPC preflight would be broken by full state archival.

With this proposal, it is vital to ensure that automatic restore does not result in a situation where large numbers
of developers circumvent preflight. Two specific features of this proposal help ensure developers are incentivized
to build preflight into their flows now such that they are not broken later by archival proofs. Specifically,
no in-memory read fees incentivize developers to minimize fees by using preflight to detect the maximum
number of entries currently live. Additionally, the requirement of `archivedEntries` vectors makes manually
constructing footprints challenging, as the developer would need to be able to distinguish live vs evicted state
to minimize fees. Given that preflighting will result in lower on-chain fees, and given that resource footprints
are somewhat challenging to create without captive-core seems to indicate that developers will preflight transactions
prior to submission as intended.

## Security Concerns

### Memory Based DOS Attack

By distinguishing between disk backed and memory backed reads at the protocol level, it is now required that
validators maintain all live Soroban state in memory. A significant increase in live Soroban state could
lead to an OOM based attack on the network.

However, this is not a significant concern. The `sorobanLiveStateTargetSizeBytes` (see [CAP-0062](cap-0062.md))
provides a soft cap on the amount of live Soroban state that can exist at any given time. This prevents any
sort of runaway memory issue. Additionally, because of the eviction process, there is a natural back pressure
applied to in-memory state, the rate at which can be controlled via network config settings. Finally, the rate
at which new state is created is also controlled via network configs, so there does not appear to be any
memory based DOS attack possible.

## Test Cases

TBD

## Implementation

TBD
