## Preamble

```
CAP: 0046-10
Title: Smart Contract Budget Metering
Working Group:
    Owner: Jay Geng <@jayz22>
    Authors: Jay Geng <@jayz22>, Graydon Hoare <@graydon>
    Consulted: Nicolas Barry <@MonsieurNicolas>, Dmytro Kozhevin <@dmkozh>
Status: Draft
Created: 2022-12-20
Discussion: TBD
Protocol version: TBD
```

## Simple Summary

This CAP defines the resources for running a smart contract, and proposes an componentized, extensible framework of metering those resources during runtime against a predetermined budget. 

## Working Group
TBD

## Motivation
Resource **metering** serves as the canonical truth of the cost of executing a smart contract on the network. It has two main goals.
- Preventing DoS attacks
- Ensuring fair and efficient resource allocation

The ledger has capacity limits. If transaction processing is allocated 1s (out of the total 5s of ledger closing time), and the max number of smart contract transactions per ledger is `X`, then the max compute time of each transaction is `1/X`, ignoring non-smart transactions and parallel execution. Furthermore, the ledger processing unit has a memory capacity, which the total amount of memory usage (host objects, linear memory) of executing the smart contract transaction set cannot exceed. The resource **budget** reflects those limits.

### Requirements

The metered costs must align closely to the true costs of running a smart contract. 
- If metering underestimates the true costs, the ledger is susceptible to DoS attack. Underestimation also include where metering fails to properly consider exploitable edge cases whose true cost is significantly higher than the average. 
- If overestimate, the ledger fail to fully utilize its capacity. 

In addition, metering must have:
- High coverage: metering needs to cover all the non-trivial work done by the host. 
- Metering needs to err on the side of worst case of the true cost.
- Metering based on the worst case must not deviate too far (10x) from the average cost.

### Design goals
- Explainability – the metering model should be simple enough to understand and to explain the cost composition of a contract. 
- Extensibility and maintainability – should be straightforward to add metering to future code. Changes in the implementation should not require rewrite of metering. Every iteration of code changes should not require complete model re-calibration. 
- Metering should be cheap – the act of meter charging should not amount to a significant cost.
- Being able to detect when metering is missing in code paths.

### Goals alignment
Aligns with the general goals of the overview [cap-0046](./cap-0046.md) as as well the fee model [cap-0047-07](./cap-0046-07.md).

## Abstract

This specification starts by defining which resource metrics are chosen to reflect the cost of running a smart contract. It then formalizes a process for breaking down the host (or any arbitrary code) into components and defines steps for deriving the cost parameters for each component. It then introduces new ledger configuration entries for storing the budget and metering parameters, and finally, discusses relevant issues regarding maintaining and upgrading these parameters. 

This CAP builds heavily on previous chapters of CAP-46. The reader needs to be comfortable with various concepts introduced in the CAP-46 (overview), 01 (the runtime environment), 03 (host functions), 09 (network configuration ledger entries). Familiarity with 05 (smart contract data) and 07 (fee model) is also beneficial. 

## Specification

Smart contract transactions on the ledger compete for 1. compute time 2. host memory. The two resource metrics we employ for budget and metering are **cpu instruction count** (`cpu_insns`) and **bytes of memory allocated** (`mem_bytes`). 

### Definitions
The entire host side code is broken down to a list of components and blocks:
- A **component** is some code whose costs can be approximated with a linear or constant function of some input value derived from the code’s inputs.
- A **block** is any code that cannot be measured this way, usually because it implements a complex or data-dependent algorithm.

Components and blocks may be wild or tame:
- Code is **wild** if it’s code we didn’t write and are not maintaining a fork of.
- Code is **tame** if it’s code we wrote or are maintaining a fork of.

### Requirements for a component
1. Depends on a single input.
2. Independent from other components.
3. The cost of each resource type - `cpu_insns` and `mem_bytes` - follows a linear or constant characteristics w.r.t. the input.

![Call tree diagram](../contents/cap-0046/0010/Call-tree-diagram.jpg)

### Call-tree invariant
Consider the host code as a tree of called blocks and components (see figure 1), with the entrypoint at the root, blocks as interior nodes and components as leafs of the tree. 

We structure the host in such a way that ensures as an **invariant** that **every component in the call tree is metered on every path to it**. This is done by ensuring the following:
- Blocks consist only of trivial (un-metered) code, calls to components, and calls to other blocks. 
- Every piece of wild component is converted to a tame component, tracked by the cost model with a unique code number assigned to it.
- Components are standalone and do not call other blocks or components — they are truly the leafs of the tree.

The full list of component types are defined in `enum ContractCostType`, see "XDR changes". 

Once the call-tree invariant is satisfied, we can ensure that if every single component is metered, the entire call-tree is metered. 

### Metering a component
During runtime, whenever a component is hit, the meter is incremented by `y = ax + b`, where `x` is the component's input, `a` and `b` are the pre-fitted linear and constant parameter of that resource type. The metering happens independently for `cpu_insns` and `mem_bytes`, so there will be two sets of parameters for each component.

To obtain the parameters, we isolate the component and set up a benchmark sandbox around it with profiling enabled (e.g. `perf_event` or `rusage`). We then call it repeatedly with varying input size, measure and record the resource output for each input size. Finally we fit a linear curve and extract the parameters. 

### Cost parameters
The result of calibration for per resource type is a set of cost parameters of size `C x 2`, where `C` is the number of cost types. The cost parameters per resource type form a `ConfigSettingEntry`. 

### The budget
The budget for each resource type is a `ConfigSettingEntry` that is determined in consensus by the validators. The budget reflects the ledger processing capacity in accordance to the requirements in the "Requirements" section. We can start with an initial `cpu_insns` budget of 4'000'000 and `mem_bytes` of 10MB. These numbers may change before this CAP finalizes.

At every metering charging, the total charges will be compared with the budget, and if exceeds, will result in a "resource budget exceeded" host error.

### XDR changes
See [cap-0046 Overview](./cap-0046-01.md), specifically the `ConfigSettingEntry` which has four new additions corresponding to budget and metering for each resource type, as well as the new file [Stellar-contract-cost-type.x](../contents/cap-0046/Stellar-contract-cost-type.x) that defines the cost types `ContractCostType` and cost parameters entry `ContractCostParamEntry`. 

### Metering an arbitrary new piece of code
The above have so far presented the definition of components, the list of components already identified in the host and how to calibrate each component to obtain the cost parameters.

The main challenge of dealing with an arbitrary new piece of code (what the host starts out to be) is to identify the components through an iterative process:
1. Break down the code into a call tree where each node consists of meaningful, non-trivial operation.
2. Identify the leaf nodes, making sure they are components according to the “requirements for a component”.
3. For any TC, meter it according to “metering a component”
4. If it contains any wild code, follow "taming wild code” to tame it. This step needs to be done in junction with 3. 
5. Start from the leaf nodes, mark them as metered, then proceed up level by level until the reaching root. 
- If a node is composed of only metered children, it is a metered block. 
- Once the root is metered, the call-tree invariant is satisfied and the entire call-tree is metered. 

### Taming wild code
As mentioned previously, one of the keys to satisfying the call-tree invariant is that all wild code, blocks or components, be tamed. This consists of the following patterns
1. A tamed block (TB) calling a tamed component (TC)
2. A wild block (WB) calling a TC
3. A TB calling a wild component (WC)
4. A TB calling a wild block (WB), where the wild block (WB) calls some other WC which we do not have access to.
For 1 and 2, metering is already covered by the TC and there is nothing else we need to do. 

For 3, we are calling a WC which is standalone and does not call us back. We can easily tame the WC by attaching a metering harness to it. 

The tricky scenario is 4, where a TB calls into a WB that calls into a mixture of WCs and TCs (if all of them are WCs, then the entire WB becomes a WC and we are in scenario 3). We have two options to deal with this scenario:
1. Approximate the WB as a new WC, using proper assumptions to separate out all of its logic dependencies from any TCs. Figure 2 illustrates this process and compares the call tree before and after. 
2. If 1 is not possible, we have to tame it the brute force way either by forking the code and modifying it, or choose a different library, or remove this functionality altogether.

![Taming wild code](../contents/cap-0046/0010/Taming-a-call-tree.jpg)

## Design Rationale

### Why `cpu_insns` metric
We use cpu instruction count as the main metrics for "compute" because it is a direct proxy to process running time, i.e. `run_time = cpu_insns_count / clock_freq / ave_insns_per_cycle`.
The average instructions per cycle `ave_insns_per_cycle` depends on a set of CPU architecture-specific factors such as the instruction set, instruction length, micro-ops, instruction-level parallelism (which depends on instruction window size, branch-prediction), which are stable per architecture. 

Assuming 2GHz cpu with an ave. insns per cycle of 2, 4'000'000 cpu instructions roughly equals 1ms.

Note that the instruction count may vary across architectures, but the metering model needs to be same across various archs, so we will need to provide a guidance on recommended setup for metering calibration.

Another considered alternative resource is execution time, which relates much closer to the actual cost in ledger closing time. However, execution time is much more volatile and less-deterministic, which make it a less desirable target metric for metering. 

### Why `mem_bytes` metric
The bytes of memory allocated is a good proxy of the memory footprint of contract execution. The majority of the smart contract memory footprint comes from 1. a fixed-sized linear memory 2. immutable host objects created during contract execution, and both of these are not freed until the end of contract execution. This memory model is very similar to the arena allocator. Using allocated memory as the metric is an worst-case approximation that is 1. close to the actual memory cost 2. gives us flexibility to switch to an actual arena allocator which would make it the actual cost.

<!-- The memory allocation is deterministic and independent of architecture. -->

### Why do we have to model the costs?
In other words, why can't we profile the contract at runtime and use the results directly for metering? Because the profiling results are non-deterministic and 1. we can't use them for consensus 2. the contract execution outcome won't be able to be replayed bit-identically. Using an analytical model ensure determinism for consensus and replayability (more on this later).

### Why linear and constant components only?
The obvious reason is simplicity. We want the costs to follow a simple linear characteristic such that we can fit it accurately without needing a complex numerical model (and fitting process, heuristics etc). 

A model with higher order dependencies also risk the worst-case costs significantly outweighing the average, and any small deviation in the input resulting in significant over or underestimation of the costs. This goes against the design goals. 


### Host vs WASM vm
This metering framework is generic and does not differentiate between the host and the WASM vm. Both the host and the vm are treated as components and blocks defined in the "specification" section and subject to the same metering procedures. 

Our current choice of the WASM virtual machine implementation is Wasmi, which is a lightweight interpreter of the wasm standard, written in the same language (Rust) as the host. Wasmi runs an inner interpreter loop that executes a single wasm instruction on each loop. Thus every wasm instruction logic fits the requirements of a component. `WasmInsnExecT0~4` in `ContractCostType` are designated for the wasm instructions (instead of having one type designated to each of the 100+ wasm instructions, we group them into tiers 0~4 where each tier of wasm instructions costs relatively the same amount of cpu insns).

We maintain a fork of Wasmi with metering added. This makes Wasmi is a tamed "wild component". 

(Note this does not mean we are tied to a particular wasm implementation, it's just an example. If we decide to switch to a different interpreter or JIT in the future, we will be able to apply the same procedure to derive a new set of metering components.)


### Relation to cap-0046-07 (fee model)

[CAP-0046-07](./cap-0046-07.md) proposed a fee model for smart contracts taking into account ledger access, storage and computation (or "gas"). This CAP details the computation aspect. However, this proposal identifies cpu and memory as separate aspects of the compute cost that needs to be budgeted separately. This difference needs to be resolved before this CAP finalize, i.e., either expand gas network settings in 07 or consolidate the `cpu_insns` and `mem_bytes` into a single "gas" parameter in here. 

### Cost estimation
This proposal relies on the "preflight" mechanism to provide users with cost estimation of a transaction. The total costs for each resource type as well as inputs to each individual cost type will be returned from the preflight simulation. These costs, however can only serve as guidance to the actual cost, since the ledger snapshot used for preflight may be outdated. Thus it is not guaranteed that a transaction staying below the budget during preflight will not exceed it during the actual run. 

## Parameters Upgrade
Both the budget and metering parameters are stored on the ledger as `ConfigLedgerEntry` and their upgrade and validation process have been discussed in [CAP-0046-09](./cap-0046-09.md). 

Scenarios when upgrade is necessary:
- New blocks has been introduced in the host that require introducing new components. Such changes include e.g. a new crypto primitive function. Note that if new block merely consist of trivial code and calling existing components, then it has no effect on metering and no upgrade is needed. 
- Changes on the host components, or version changes in its dependencies (e.g. Rust) that result in observable difference in components' cost characteristics. In rare cases, if the cost characteristics becomes no longer linear, then the component needs to be broken down into finer sub-components. See "Taming wild code" section above.

### The “metered” stamp
We may need to introduce a new mechanism for stamping the metered entities in the host, following the definitions of wild/tamed components/blocks outlined in previous section. Such a mechanism would help us ensuring the call-tree invariant is satisfied by examining the root block. A further mechanism to automatically detect if metering is missing on a path would be even more ideal. 

We will also need to introduce set of reviewing standards that differentiates between block vs component changes. A metered component is subject to significantly higher bars for review and audit, to make sure the component criteria are truly satisfied, as they are the foundational building blocks of the budget metering framework.

## Open Issues

### Maintainability
The cost parameters need to be maintained to prevent the metering model from gradually deviating away from reality (model drift). Even if we maintain the same host unchanged, the host's dependencies may change that result in small performance differences which can accumulate over time, causing the cost models to drift. To combat that, we will need to publish a set of specs where the metering calibration benchmark needs to be run regularly, along with a suite of tests  and criteria for determining when the model parameters need to be updated. 

### Replayability
The general issue of versioning will be covered by a different CAP. 

Although the metering model is deterministic, the model inputs might change due to a change in software version, making it unable to replay an older ledger and produce identical metering results with a newer software version. 

To combat that, we would have to maintain every version of host (with its particular set of dependencies) simultaneously all the time, and pick the right version to replay a particular older ledger. This is cumbersome and likely unrealistic. An alternative is to record the metering results along with the contract execution results, and during replay take the metering results as truth and only replay the transaction to get the contract execution results. 

## Security Concerns
Missed or inaccurate metering can cause security concerns in two aspects:
- **Denial of Service**: the computed costs significantly underestimate the true cost of running a contract, this can slowdown the validators and prevent them to close the ledger in an acceptable time frame.
- **Under-Utilization of the Ledger Capacity**: this is not a direct attack per se. However, a side effect of overestimation in metering, is the ledger could be filled with many (deliberately crafted) fast contract transactions which theoretically could require more resource at the worst case, causing the ledger to be under-utilized. This may in turn cause other (important) transactions to queue up and not making into the ledger in a reasonable time. 

## Implementation
The budget and metering, calibration has been implemented in the host, primarily:
- [PR 118](https://github.com/stellar/rs-soroban-env/pull/118) contains the initial budget and metering framework
- [PR 307](https://github.com/stellar/rs-soroban-env/pull/307) more comprehensive coverage of metering
- [PR 561](https://github.com/stellar/rs-soroban-env/pull/561) adds the calibration framework
- [PR 597](https://github.com/stellar/rs-soroban-env/pull/597) calibration for wasm instructions

in Wasmi (our fork of the Wasm interpreter):
- [PR 1](https://github.com/stellar/wasmi/pull/1)
- [PR 10](https://github.com/stellar/wasmi/pull/10)

and in the sdk:
- [PR 789](https://github.com/stellar/rs-soroban-sdk/pull/789)

The stellar-core side implementation has not been done yet.