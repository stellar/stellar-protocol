## Preamble

```
CAP: 0046-10
Title: Smart Contract Budget Metering
Working Group:
    Owner: Jay Geng <@jayz22>, Graydon Hoare <@graydon>
    Authors: Jay Geng <@jayz22>
    Consulted: Nicolas Barry <@MonsieurNicolas>, Dmytro Kozhevin <@dmkozh>
Status: Draft
Created: 2022-12-20
Discussion: TBD
Protocol version: TBD
```

## Simple Summary

This CAP defines the resources for running a smart contract, and proposes an componentized, extensible framework of metering those resources during runtime against a predetermined budget. 

## Working Group
TBD

## Motivation
Resource **metering** serves as the canonical truth of the cost of executing a smart contract on the network. It has two main goals.
- Preventing DoS attacks
- Ensuring fair and efficient resource allocation

The ledger has capacity limits. If transaction processing is allocated 1s (out of the total 5s of ledger closing time), and the max number of smart contract transactions per ledger is `X`, then the max compute time of each transaction is `1/X`, ignoring non-smart transactions and parallel execution. Furthermore, the ledger processing unit has a memory capacity, which the total amount of memory usage (host objects, linear memory) of executing the smart contract transaction set cannot exceed. The resource **budget** reflects those limits.

### Requirements

The metered costs must align closely to the true costs of running a smart contract. 
- If metering underestimates the true costs, the ledger is susceptible to DoS attack. Underestimation also include where metering fails to properly consider exploitable edge cases whose true cost is significantly higher than the average. 
- If overestimate, the ledger fail to fully utilize its capacity. 

In addition, metering must have:
- High coverage: metering needs to cover all the non-trivial work done by the host. 
- Metering needs to err on the side of worst case of the true cost.
- Metering based on the worst case must not deviate too far (10x) from the average cost.

### Design goals
- Explainability – the metering model should be simple enough to understand and to explain the cost composition of a contract. 
- Extensibility and maintainability – should be straightforward to add metering to future code. Changes in the implementation should not require rewrite of metering. Every iteration of code changes should not require complete model re-calibration. 
- Metering should be cheap – the act of meter charging should not amount to a significant cost.
- Being able to detect when metering is missing in code paths.

### Goals alignment
Aligns with the general goals of the overview [cap-0046](./cap-0046.md) as as well the fee model [cap-0047-07](./cap-0046-07.md).

## Abstract

This specification starts by defining which resource metrics are chosen to reflect the cost of running a smart contract. It then formalizes a process for breaking down the host (or any arbitrary code) into components and defines steps for deriving the cost parameters for each component. It then introduces new ledger configuration entries for storing the budget and metering parameters, and finally, discusses relevant issues regarding maintaining and upgrading these parameters. 

This CAP builds heavily on previous chapters of CAP-46. The reader needs to be comfortable with various concepts introduced in the CAP-46 (overview), 01 (the runtime environment), 03 (host functions), 09 (network configuration ledger entries). Familiarity with 05 (smart contract data) and 07 (fee model) is also beneficial. 

## Specification

Smart contract transactions on the ledger compete for 1. compute time 2. host memory. The two resource metrics we employ for budget and metering are **cpu instruction count** (`cpu_insns`) and **bytes of memory allocated** (`mem_bytes`). 

### Definitions
The entire host side code is broken down to a list of components and blocks:
- A **component** is some code whose costs can be approximated with a linear or constant function of some input value derived from the code’s inputs.
- A **block** is any code that cannot be measured this way, usually because it implements a complex or data-dependent algorithm.

Components and blocks may be wild or tame:
- Code is **wild** if it’s code we didn’t write and are not maintaining a fork of.
- Code is **tame** if it’s code we wrote or are maintaining a fork of.

### Requirements for a component
1. Depends on a single input.
2. Independent from other components.
3. The cost of each resource type - `cpu_insns` and `mem_bytes` - follows a linear or constant characteristics w.r.t. the input.

![Call tree diagram](../contents/cap-0046/0010/Call-tree-diagram.jpg)

### Call-tree invariant
Consider the host code as a tree of called blocks and components (see figure 1), with the entrypoint at the root, blocks as interior nodes and components as leafs of the tree. 

We structure the host in such a way that ensures as an **invariant** that **every component in the call tree is metered on every path to it**. This is done by ensuring the following:
- Blocks consist only of trivial (un-metered) code, calls to components, and calls to other blocks. 
- Every piece of wild component is converted to a tame component, tracked by the cost model with a unique code number assigned to it.
- Components are standalone and do not call other blocks or components — they are truly the leafs of the tree.

The full list of component types are defined in `enum ContractCostType`, see "XDR changes". 

Once the call-tree invariant is satisfied, we can ensure that if every single component is metered, the entire call-tree is metered. 

### Metering a component
During runtime, whenever a component is hit, the meter is incremented by `y = ax + b`, where `x` is the component's input, `a` and `b` are the pre-fitted linear and constant parameter of that resource type. The metering happens independently for `cpu_insns` and `mem_bytes`, so there will be two sets of parameters for each component.

To obtain the parameters, we isolate the component and set up a benchmark sandbox around it with profiling enabled (e.g. `perf_event` or `rusage`). We then call it repeatedly with varying input size, measure and record the resource output for each input size. Finally we fit a linear curve and extract the parameters. 

### Cost parameters
The result of calibration for per resource type is a set of cost parameters of size `C x 2`, where `C` is the number of cost types. The cost parameters per resource type form a `ConfigSettingEntry`. 

### The budget
The budget for each resource type is a `ConfigSettingEntry` that is determined in consensus by the validators. The budget reflects the ledger processing capacity in accordance to the requirements in the "Requirements" section. We can start with an initial `cpu_insns` budget of 4'000'000 and `mem_bytes` of 10MB. These numbers may change before this CAP finalizes.

At every metering charging, the total charges will be compared with the budget, and if exceeds, will result in a `SCEC_EXCEEDED_LIMIT` host error.

### XDR changes
See [cap-0046 Overview](./cap-0046-01.md), specifically the `ConfigSettingEntry` which has four new additions corresponding to budget and metering for each resource type, as well as the new file [Stellar-contract-cost-type.x](../contents/cap-0046/Stellar-contract-cost-type.x) that defines the cost types `ContractCostType` and cost parameters entry `ContractCostParamEntry`. 

### Metering an arbitrary new piece of code
The above have so far presented the definition of components, the list of components already identified in the host and how to calibrate each component to obtain the cost parameters.

The main challenge of dealing with an arbitrary new piece of code (what the host starts out to be) is to identify the components through an iterative process:
1. Break down the code into a call tree where each node consists of meaningful, non-trivial operation.
2. Identify the leaf nodes, making sure they are components according to the “requirements for a component”.
3. For any TC, meter it according to “metering a component”
4. If it contains any wild code, follow "taming wild code” to tame it. This step needs to be done in junction with 3. 
5. Start from the leaf nodes, mark them as metered, then proceed up level by level until the reaching root. 
- If a node is composed of only metered children, it is a metered block. 
- Once the root is metered, the call-tree invariant is satisfied and the entire call-tree is metered. 

### Taming wild code
As mentioned previously, one of the keys to satisfying the call-tree invariant is that all wild code, blocks or components, be tamed. This consists of the following patterns
1. A tamed block (TB) calling a tamed component (TC)
2. A wild block (WB) calling a TC
3. A TB calling a wild component (WC)
4. A TB calling a wild block (WB), where the wild block (WB) calls some other WC which we do not have access to.
For 1 and 2, metering is already covered by the TC and there is nothing else we need to do. 

For 3, we are calling a WC which is standalone and does not call us back. We can easily tame the WC by attaching a metering harness to it. 

The tricky scenario is 4, where a TB calls into a WB that calls into a mixture of WCs and TCs (if all of them are WCs, then the entire WB becomes a WC and we are in scenario 3). We have two options to deal with this scenario:
1. Approximate the WB as a new WC, using proper assumptions to separate out all of its logic dependencies from any TCs. Figure 2 illustrates this process and compares the call tree before and after. 
2. If 1 is not possible, we have to tame it the brute force way either by forking the code and modifying it, or choose a different library, or remove this functionality altogether.

![Taming wild code](../contents/cap-0046/0010/Taming-a-call-tree.jpg)

## Design Rationale

### Why `cpu_insns` metric
We use cpu instruction count as the main metrics for "compute" because it is a direct proxy to process running time, i.e. `run_time = cpu_insns_count / clock_freq / ave_insns_per_cycle`.
The average instructions per cycle `ave_insns_per_cycle` depends on a set of CPU architecture-specific factors such as the instruction set, instruction length, micro-ops, instruction-level parallelism (which depends on instruction window size, branch-prediction), which are stable per architecture. 

Assuming 2GHz cpu with an ave. insns per cycle of 2, 4'000'000 cpu instructions roughly equals 1ms.

Note that the instruction count may vary across architectures, but the metering model needs to be same across various archs, so we will need to provide a guidance on recommended setup for metering calibration.

Another considered alternative resource is execution time, which relates much closer to the actual cost in ledger closing time. However, execution time is much more volatile and less-deterministic, which make it a less desirable target metric for metering. 

### Why `mem_bytes` metric
The bytes of memory allocated is a good proxy of the memory footprint of contract execution. The majority of the smart contract memory footprint comes from 1. a fixed-sized linear memory 2. immutable host objects created during contract execution, and both of these are not freed until the end of contract execution. This memory model is very similar to the arena allocator. Using allocated memory as the metric is an worst-case approximation that is 1. close to the actual memory cost 2. gives us flexibility to switch to an actual arena allocator which would make it the actual cost.

<!-- The memory allocation is deterministic and independent of architecture. -->

### Why do we have to model the costs?
In other words, why can't we profile the contract at runtime and use the results directly for metering? Because the profiling results are non-deterministic and 1. we can't use them for consensus 2. the contract execution outcome won't be able to be replayed bit-identically. Using an analytical model ensure determinism for consensus and replayability (more on this later).

### Why linear and constant components only?
The obvious reason is simplicity. We want the costs to follow a simple linear characteristic such that we can fit it accurately without needing a complex numerical model (and fitting process, heuristics etc). 

A model with higher order dependencies also risk the worst-case costs significantly outweighing the average, and any small deviation in the input resulting in significant over or underestimation of the costs. This goes against the design goals. 


### Host vs WASM vm
This metering framework is generic and does not differentiate between the host and the WASM vm. Both the host and the vm are treated as components and blocks defined in the "specification" section and subject to the same metering procedures. 

Our current choice of the WASM virtual machine implementation is Wasmi, which is a lightweight interpreter of the wasm standard, written in the same language (Rust) as the host. Wasmi runs an inner interpreter loop that executes a single wasm instruction on each loop. Thus every wasm instruction logic fits the requirements of a component. `WasmInsnExecT0~4` in `ContractCostType` are designated for the wasm instructions (instead of having one type designated to each of the 100+ wasm instructions, we group them into tiers 0~4 where each tier of wasm instructions costs relatively the same amount of cpu insns).

We maintain a fork of Wasmi with metering added. This makes Wasmi is a tamed "wild component". 

(Note this does not mean we are tied to a particular wasm implementation, it's just an example. If we decide to switch to a different interpreter or JIT in the future, we will be able to apply the same procedure to derive a new set of metering components.)


### Relation to cap-0046-07 (fee model)

[CAP-0046-07](./cap-0046-07.md) proposed a fee model for smart contracts taking into account ledger access, storage and computation (or "gas"). This CAP details the computation aspect. However, this proposal identifies cpu and memory as separate aspects of the compute cost that needs to be budgeted separately. This difference needs to be resolved before this CAP finalize, i.e., either expand gas network settings in 07 or consolidate the `cpu_insns` and `mem_bytes` into a single "gas" parameter in here. 

### Cost estimation
This proposal relies on the "preflight" mechanism to provide users with cost estimation of a transaction. The total costs for each resource type as well as inputs to each individual cost type will be returned from the preflight simulation. These costs, however can only serve as guidance to the actual cost, since the ledger snapshot used for preflight may be outdated. Thus it is not guaranteed that a transaction staying below the budget during preflight will not exceed it during the actual run. 

## Parameters Upgrade
Both the budget and metering parameters are stored on the ledger as `ConfigLedgerEntry` and their upgrade and validation process have been discussed in [CAP-0046-09](./cap-0046-09.md). In general, the parameters can be upgraded with or without a protocol version upgrade. 

In the case of a protocol version upgrade, here are the scenarios where the parameters also has to be upgraded:
- New blocks have been introduced in the host that require introducing new components. Such changes include e.g. a new crypto primitive function. Note that if a new block merely consists of trivial code and calling existing components, then it has no effect on metering and no upgrade is needed. 
- Changes on the host components, or version changes in its dependencies (e.g. Rust) that result in observable difference in components' cost characteristics. In rare cases, if the cost characteristics becomes no longer linear, then the component needs to be broken down into finer sub-components. See "Taming wild code" section above.

### The “metered” stamp
We may need to introduce a new mechanism for stamping the metered entities in the host, following the definitions of wild/tamed components/blocks outlined in previous section. Such a mechanism would help us ensuring the call-tree invariant is satisfied by examining the root block. A further mechanism to automatically detect if metering is missing on a path would be even more ideal. 

We will also need to introduce set of reviewing standards that differentiates between block vs component changes. A metered component is subject to significantly higher bars for review and audit, to make sure the component criteria are truly satisfied, as they are the foundational building blocks of the budget metering framework.

## Open Issues

### Maintainability
The cost parameters need to be maintained to prevent the metering model from gradually deviating away from reality (model drift). Even if we maintain the same host unchanged, the host's dependencies may change that result in small performance differences which can accumulate over time, causing the cost models to drift. To combat that, we will need to publish a set of specs where the metering calibration benchmark needs to be run regularly, along with a suite of tests  and criteria for determining when the model parameters need to be updated. 

### Versioning and Replayability

Although the metering models are deterministic, the model inputs may vary across different software versions. For example, consider a third-party library routine that calls our host object comparison component `obj_compare` for an unknown number of times. The metering of that routine is therefore delegated to `obj_compare`. If a software upgrade happens to the routine which results in the number of `obj_compare` call to be increased from `N` to `N+1`, the cost will be different (which may effect the success-or-failure status of a contract) even though no other observable difference exists. In other words, due to the intricate relations between metering logic and the code logic under execution, the surface area of observable differences between a transaction's execution and its replay have been enlarged. 

This is not a problem for consensus, as long as all the validators maintain the exact same software version. There are two options to solve the replay problem:
1. Maintain multiple software versions simultaneously. For an old protocol version, its exact host software version needs to be included in the current stellar-core. A version map between protocol version and the host software version needs to be maintained and looked up during replay. In practice, the number of software versions could be less than the number of protocol versions, since a protocol version upgrade may not result in observable differences in any of the transactions' replay between the old and the new version. In which case, the older software version can be retired and replaced by the newer version, but this is more of an exception. 
2. Make the cost results irrelevant in replay. In other words, relax the bit-identicalness requirement for contract execution costs. During replay:
    - On a successful SC transaction, take the fee due to contract execution (cpu and memory costs) as the "truth" (in order to produce the correct hash), and ignore the metering logic which arrive to those results. Also set the budget to unlimited so the replay transaction cannot fail due to out of budget. 
    - On a failed SC transaction, skip the transaction. A failed SC transaction ought to not have any side effects, so that it is safe to be skipped. 

The main pro of option 1 is that it preserves the bit-identicalness property of replay, however, at the cost of increased maintenance burden. 

The rationale for option 2, besides easier software maintenance, is that the accounting logic of metering should not have any significance besides arriving at the success-or-failure status and the fee charged of a contract transaction. In other words, no other side effects should be produced as a result of metering that is relevant to the observable outcomes of a transaction, thus justifies the choice of skipping the metering process altogether during replay. 

However, there are several cons of option 2:
- Adds the limitation that metering cannot produce any side effect besides the execution cost numbers, which must be the end results in all current and future transactions. This prohibits the possibility that a contract transaction relies on intermediate execution cost results as part of its logic, such as deciding whether or not to call another contract based on how much budget it has remaining.
- Places a dependency of replay on the transaction results.  
- The budget metering is just a special case in the broader issue of host software versioning. Even without budget metering, the surface area of potential differences between a live execution and its replay is already large and unpredictable, thus necessitates host multi-versioning. The budget metering just increases such surface area. 

Based on above concerns, option 1 is likely the preferred option. 

The broader issue of host software versioning will be discussed in a different chapter and must be finalized before this CAP finalizes.

## Security Concerns
Missed or inaccurate metering can cause security concerns in two aspects:
- **Denial of Service**: the computed costs significantly underestimate the true cost of running a contract, this can slowdown the validators and prevent them to close the ledger in an acceptable time frame.
- **Under-Utilization of the Ledger Capacity**: this is not a direct attack per se. However, a side effect of overestimation in metering, is the ledger could be filled with many (deliberately crafted) fast contract transactions which theoretically could require more resource at the worst case, causing the ledger to be under-utilized. This may in turn cause other (important) transactions to queue up and not making into the ledger in a reasonable time. 

## Implementation
The budget and metering, calibration has been implemented in the host, primarily:
- [PR 118](https://github.com/stellar/rs-soroban-env/pull/118) contains the initial budget and metering framework
- [PR 307](https://github.com/stellar/rs-soroban-env/pull/307) more comprehensive coverage of metering
- [PR 561](https://github.com/stellar/rs-soroban-env/pull/561) adds the calibration framework
- [PR 597](https://github.com/stellar/rs-soroban-env/pull/597) calibration for wasm instructions

in Wasmi (our fork of the Wasm interpreter):
- [PR 1](https://github.com/stellar/wasmi/pull/1)
- [PR 10](https://github.com/stellar/wasmi/pull/10)

and in the sdk:
- [PR 789](https://github.com/stellar/rs-soroban-sdk/pull/789)

The stellar-core side implementation has not been done yet.