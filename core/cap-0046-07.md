```
CAP: 0046-07 (formerly 0055)
Title: Fee model in smart contracts
Working Group:
    Owner: MonsieurNicolas
    Authors: dmkozh
    Consulted:
Status: Draft
Created: 2022-06-03
Discussion: TBD
Protocol version: TBD
```

## Simple Summary

This CAP defines the mechanism used to determine fees when using smart contracts on the Stellar network.

## Working Group

TBD

## Motivation

With the introduction of smart contracts on the network, the existing fee model of the "classic" transaction system is too simplistic: it requires careful design of the code that runs "on chain" as to ensure that all operations have a similar cost and performance profile, which is not possible with arbitrary code running in contracts.

### Goals Alignment

Goals of the updated fee model are to:
* ensure fairness between users and use cases.
* promote scalable patterns on the network, doing more with the same amount of overall resources.
* ensure that the network operates in a sustainable way, network operators should be in control of their operating cost.

## Abstract

This CAP proposes various network level parameters (voted on by validators), and fee structure for the different kinds of resources involved on the network.

The fee structure is designed to discourage "spam" traffic and overall waste of infrastructure capacity.

## Specification

### XDR changes

The following network parameters are introduced (in some cases increments are used to mitigate for rounding errors):

```
// general “smart contract execution lane” settings
struct ContractExecutionLanesSettingsV0
{
    uint32 ledgerMaxTxCount; // maximum number of “smart” transactions per ledger
};

// instruction count aka "compute" settings
struct ContractInstructionsNetworkSettingsV0
{
    int64 ledgerMaxInstructions; // maximum instructions per ledger
    int64 txMaxInstructions; // maximum instructions per transaction
    int64 feeRatePerInstructionsIncrement; // cost of INSTRUCTIONS_INCREMENT=10k instructions
};

// Ledger access settings
struct ContractLedgerCostNetworkSettingsV0
{
    uint32 ledgerMaxReadLedgerEntries;// maximum number of ledger entry read operations per ledger
    uint32 ledgerMaxReadBytes; // maximum number of bytes that can be read per ledger
    uint32 ledgerMaxWriteLedgerEntries;// maximum number of ledger entry write operations per ledger
    uint32 ledgerMaxWriteBytes; // maximum number of bytes that can be written per ledger

    uint32 txMaxReadLedgerEntries;// maximum number of ledger entry read operations per transaction
    uint32 txMaxReadBytes; // maximum number of bytes that can be read per transaction
    uint32 txMaxWriteLedgerEntries;// maximum number of ledger entry write operations per transaction
    uint32 txMaxWriteBytes; // maximum number of bytes that can be written per transaction


    int64 feeReadLedgerEntry; // fee per ledger entry read
    int64 feeWriteLedgerEntry; // fee per ledger entry write

    int64 feeRead1KB; // fee for reading 1KB
    int64 feeWrite1KB; // fee for writing 1KB

    int64 bucketListSizeBytes; // bucket list fees grow slowly up to that size
    int64 bucketListFeeRateLow; // fee rate in stroops when the bucket list is empty
    int64 bucketListFeeRateHigh; // fee rate in stroops when the bucket list reached bucketListSizeBytes
    uint32 bucketListGrowthFactor; // rate multiplier for any additional data passed the first bucketListSizeBytes
};

// historical data (pushed to core archives) settings
struct ContractHistoricalNetworkSettingsV0
{
    int64 feeHistorical1KB; // fee for storing 1KB in archives
};

// Meta data (pushed to downstream systems) settings
struct ContractMetaDataNetworkSettingsV0
{
    uint32 txMaxExtendedMetaDataSizeBytes; // maximum size of extended meta data produced by a transaction
    int64 feeExtendedMetaData1KB; // fee for generating 1KB of extended meta data
};

// Bandwidth related data settings
struct ContractBandwidthDataNetworkSettingsV0
{
    uint32 ledgerMaxPropagateSizeBytes; // maximum size in bytes to propagate per ledger
    uint32 txMaxSizeBytes; // maximum size in bytes for a transaction

    int64 feePropagateData1KB; // fee for propagating 1KB of data
};

```

Additional changes at the Transaction/TransactionSet level:

```
// Transaction changes (actual diff TBD)

   int64 fee; // total fee for this transaction
   int64 refundableFee; // portion of `fee` allocated to other refundable fees

// Additional properties:
   uint32 instructions; // how many instructions are needed for this tx

   uint32 readBytes; // how many bytes will be read by this tx
   uint32 writeBytes; // how many bytes will be written by this tx

   uint32 extendedMetaDataSizeBytes; // how many bytes can be added to the meta by this tx
   
// TransactionEnvelope changes

// SCP value: hash of GeneralizedTransactionSet before and after removing payload


// GeneralizedTransactionSet changes TBD, need a way to express different fee markets

case TXSET_COMP_SMART_TXS_MAYBE_DISCOUNTED_FEE:
  struct
  {
    int64* inclusionFee;
    TransactionEnvelope txs<>;
  } smartTxsMaybeDiscountedFee;

// Other constants

   const TX_BASE_RESULT_SIZE = 300; // the approximation to use for `TransactionResult` when pushing to archive
```

Changes to ledger data:
```

TBD: `LedgerEntry` with expiration date

```

### Semantics

Validity constraints:
* source account must be able to pay for the total fee bid `tx.fee` for that transaction.
* the number of smart contract transactions cannot exceed `ledgerMaxTxCount` per ledger

#### Resources with contention and inclusion fees

The approach taken in this proposal is to decompose fees into:
* `resourcesFee` - derived from how much resources a transaction is using and network parameters that evolve with time as to price those resources appropriately
* `inclusionFeeBid` - this is the "social value" part of the fee, it represents the intrinsic value that the submitter puts on that transaction.

The resources where we allow competition are:
* instructions (virtual instructions to execute)
* ledger data access (bytes transferred)
* network propagation (bandwidth)

Transactions will have to pay both for `resourcesFee` and `inclusionFeeBid`, with `inclusionFeeBid` used to prioritize transactions relative to each other (both when flooding and for inclusion in transaction sets).

#### TransactionSet semantics

While transactions bid specific `inclusionFeeBid`, the effective bid may be lowered within a transaction set component by setting `inclusionFee`.

When set:
* all transactions within the component must bid more than `inclusionFee`, ie for each transaction `inclusionFeeBid >= inclusionFee`
* the effective inclusion bid for transactions in that group is `inclusionFee`

#### Fee computation while applying transactions

As in classic, total fees are taken from the source account balance before applying transactions.

Note that the total fee charged is equal to
`tx.fee + inclusionFee - inclusionFeeBid` as to accomodate for potential inclusion fee discounts.

At the end of the transaction execution, refund the source account for resources that are eligible for a refund (this refund is reflected under `txChangesAfter` in the meta).

Right now the following resources fall under this category:
  * extended meta data

The total fee `tx.fee`, `tx.refundableFee` and other fees are connected in the following way:

`tx.fee =  resourcesFee(tx) + inclusionFeeBid + tx.refundableFee`

with
`
resourcesFee(tx) = Instructions_fee(tx) + LedgerDataAccess_fee(tx) +
    NetworkData_fee(tx) + historical_flat_fee(txEnvelope)
`

#### Execution time

A transaction contains:
* how many "Instructions" they want to bid for. `uint32 Tx.instructions`.

`Instructions_fee(Tx) = round_up(Tx.instructions*feeRatePerInstructionsIncrement/INSTRUCTIONS_INCREMENT)`

Validity constraints:
* per transaction
    * `Tx.instructions <= txMaxInstructions`.
* ledger wide (`GeneralizedTransactionSet`)
  * sum of all `tx.instructions` <= `ledgerMaxInstructions`.

#### Ledger data

A transaction contains:
* the read `tx.LedgerFootprint.readOnly` and read/write `tx.LedgerFootprint.readWrite` sets (ledger keys).
* the maximum total amount of data `uint32 tx.readBytes` that gets read by `tx.LedgerFootprint.readOnly` and `tx.LedgerFootprint.readWrite`.
* the maximum total amount of data `uint32 tx.writeBytes` that can be written by `tx.LedgerFootprint.readWrite`.


```
LedgerDataAccess_fee(tx) =
  (length(tx.LedgerFootprint.readOnly)+length(tx.LedgerFootprint.readWrite))*feeReadLedgerEntry + // cost of reading ledger entries
  length(tx.LedgerFootprint.readWrite)*feeWriteLedgerEntry + // cost of writing ledger entries
  round_up(tx.readBytes * feeRead1KB / 1024) + // cost of processing reads
  round_up(wfee_rate(lcl.BucketListSize)* tx.writeBytes)) // cost of adding to the bucket list
```

With

```
wfee_rate(s) = (bucketListFeeRateHigh - bucketListFeeRateLow)*s/bucketListSizeBytes +
bucketListFeeRateLow +
(if s > bucketListSizeBytes,
    bucketListGrowthFactor*
    (bucketListFeeRateHigh - bucketListFeeRateLow)*
    (s-bucketListSizeBytes)/bucketListSizeBytes,
    0)
```

__Open:__ `wfee_rate` (and possibly `LedgerDataAccess_fee`) needs to be reconciled with "rent" (not finalized at the time of this writing) as "paying rent" is very similar to adding an entry to the ledger. The difference is that adding an entry to the ledger adds the entry to the "topmost" bucket, where as paying rent is adding an entry to a bucket while merging.

Validity constraints:
* per transaction
   * `length(tx.LedgerFootprint.readOnly) <= txMaxReadLedgerEntries`.
   * `tx.readBytes <= txMaxReadBytes`.
   * `length(tx.LedgerFootprint.readWrite) <= txMaxWriteLedgerEntries`.
   * `tx.writeBytes <= txMaxWriteBytes`.
* ledger wide (`GeneralizedTransactionSet`)
   * `sum(length(tx.LedgerFootprint.readOnly) + length(tx.LedgerFootprint.readWrite)) <= ledgerMaxReadLedgerEntries`.

#### Historical storage

`historical_flat_fee(txEnvelope) = round_up((size(txEnvelope)+TX_BASE_RESULT_SIZE) * feeHistorical1KB / 1024)`

Where `TX_BASE_RESULT_SIZE` is a constant approximating the size in bytes of transaction results published to archives.

Validity constraints:
_None_

#### Extended meta data

__open:__ we could consider removing `tx.extendedMetaDataSizeBytes` and instead just use `txMaxExtendedMetaDataSizeBytes`.
There are a couple potential problems:
* this may artifically increase the minimum account balance required to submit a transaction
* transactions may fail if the network votes to increase `txMaxExtendedMetaDataSizeBytes` (this problem may have to be solved in general anyways)

A transaction contains:
* `uint32 tx.extendedMetaDataSizeBytes` the maximum size of extended data produced by this transaction

`extendedMetaData_flat_fee(tx) = round_up(tx.extendedMetaDataSizeBytes * feeExtendedData1KB / 1024)`

Validity constraints:
* per transaction
  * `tx.extendedMetaDataSizeBytes <= txMaxExtendedMetaDataSizeBytes`

#### Bandwidth related

A transaction contains:
* implicitely, its impact in terms of bandwidth utilization, the size (in bytes) of the `TransactionEnvelope`

`NetworkData_fee(tx) = round_up(size(txEnvelope) * feePropagateData1KB / 1024)`

Validity constraints:
* per transaction
  * `size(txEnvelope) <= txMaxTxSizeBytes`
* ledger wide
  * sum of all `size(txEnvelope)` <= `ledgerMaxPropagateSizeBytes`.

#### Refundable resource fee

A transaction contains:
* `int64 tx.refundableFee` fee shared by all “flat fee” resources

`tx.refundableFee` must be greater than
`extendedMetaData_flat_fee(tx)`

## "Fee bump" and failing transactions

__Open:__
The high number of settings involved in making a transaction succeed at runtime may cause additional usability issues.

Transactions may fail due to variance in behavior (similar to how footprints can change): underestimating any of the transaction level field may cause the transaction to be included in a ledger, but fail later on (causing the sequence number to be consumed and fees burned).

There might be a need for a "fee bump" wrapper transaction of sorts that takes the burden in case of failure, and that allows to override the problematic fields (including footprints and any limit and fee related fields).

Unlike "fee bump", the outer "bump" transaction would have its own sequence number/fees.


## Design Rationale

### Fee estimation

This proposal relies heavily on the existence of a "preflight" mechanism to determine all parameters needed to compute fees.

Additional logic (not covered in this CAP), will be needed to determine the market rate of resources based for example on historical data (see below).


### Resources

Fees are used to ensure fair and balanced utilization of resources.

For each resource type, we're assuming a model where we can define:
* the maximum resource consumption for a transaction, as to protect the network.
* a reasonable price for any given transaction, as to ensure that there are no broken markets
* additional constraints may include
    * a "ledger wide" maximum as to protect the network and downstream systems when producing blocks.
    * "execution lane" maximum, as to ensure that execution lanes (executed in parallel), are balanced. This CAP does not attempt to define actual semantics or fee models related to parallel execution, and is mentioned here for context.

We’re also assuming that resource allocation is done independently of “classic” transactions (ie: the amount of resources allocated to smart contract execution is independent of other traffic). This points to “smart contract transactions” being managed as their own “phase” (in `GeneralizedTransactionSet` terminology) and having its own dedicated capacity expressed in terms of transactions (`ledgerMaxTxCount`).

Reasonable fees should be more than some minimum (on top of "on chain market dynamics") both to combat "spam" transactions and ensure that there is no strange incentive to perform certain operations on chain instead of performing them on other systems with worse properties (like centralized cloud infrastructure).

Validators are expected to vote regularly (once a quarter for example) to ensure that fees are set correctly for the broader ecosystem. The exact way fee parameters are established is outside the scope of this document.

#### Compute

[CAP-0046: WebAssembly Smart Contract Runtime Environment](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-01.md) introduces the notion of virtual instructions. In the context of this CAP, the only thing that matters is that an "instruction" represents an arbitrary base unit for "execution time".

As a consequence, the "goal" for validators is to construct a `GeneralizedTransactionSet` that uses up to `lcl.ContractNetworkSettingsV0.ledgerMaxInstructions`.

#### Ledger data

##### Read traffic

Reads are logically performed *before* transaction execution.

When performing reads of a ledger entry:
* The ledger entry needs to be located via some index in the ledger and the entry loaded. Depending on the underlying database technology, this translates to at least 1 disk operation.
* The bucket entry needs to be xdr decoded.

The resources to allocate in this context are therefore:
* a maximum number of ledger entry read operations in a ledger `ledgerMaxReadLedgerEntries`.
* a maximum number of bytes that can be read in a ledger `ledgerMaxReadBytes`.

The cost of a "ledger entry read" is fairly open ended, and depends on many variables. In this proposal, we give it a "base cost" for simplicity even if it translates to multiple disk operations (which is typically the case when using B-Trees for example, or if the ledger entry is retrieved by lookup over multiple buckets).

That "base cost" is defined by validators as `feeReadLedgerEntry`. This proposal does not let transactions compete directly on the number of ledger entry read operations, therefore the cost of a read operation is `feeReadLedgerEntry` (validators must still construct transaction sets that keep the number of reads below a maximum).

Market dynamics are limited to the number of bytes (see below).

Transactions contain the total number of bytes that they will read from the bucket list as well at a fee bid for reading those bytes.

The number of bytes read corresponds to the size of the latest `BucketEntry` for that ledger entry (and does not take into account the possibility that an implementation may read stale entries in buckets or may have to read other entries from a bucket).

The fee is determined based on the rate `feeRead1KB` expressed for reading 1 KB (1024 bytes) worth of data.

As transactions compete for the total read capacity `ledgerMaxReadBytes` for a given ledger, the effective fee goes up.


##### Write traffic and ledger size

Writes are performed *after* transaction execution, and are blocking the actual closing of a ledger.

When writing a ledger entry:
* The bucket entry is marshaled to binary.
* The bucket entry is appended to the topmost bucket serially.
* The bucket entry is read, hashed and written back with every level merge operation.

In this proposal, we're modeling "worst case": a bucket entry gets added to the bucket list and has to travel all the way to the bottom bucket, contributing as many bytes as the bucket entry itself.

In that case, the overhead is dominated by the size of buckets and bucket entries, and the number of bucket entries is not really a factor when merging.

Consequently, we can model the cost of a write as an append to the overall bucket list and charge a "base rate" for adding a bucket entry.

For allocating ledger entry writes, the model is analogous to "reads": a ledger is constructed as to not exceed `ledgerMaxWriteLedgerEntry` writes and each write contributes `feeWriteLedgerEntry` to the overall fee for that transaction (no market dynamics here).

As for "bytes written", the model that was chosen is:
* use the total bucket list size as the main resource to track.
* a cost function, allows to price the cost of expanding ledger size.
* ledger size, and therefore price of storage, goes down as bucket entries get merged/deleted.

The cost function that was selected is similar to what was proposed in Ethereum's [make EIP 1559 more like an AMM curve](https://ethresear.ch/t/make-eip-1559-more-like-an-amm-curve/9082).

The main point being that the fee for adding `b` bytes to a bucket list of size `s` is calculated as `fee(b,s) = lfee(s + b) - lfee(s)`, where `lfee` is the "total cost to build a bucket list of a given size".
When designing for specific properties of that function, it's useful to see the "fee rate": `fee_rate(s) = lim b->0, fee(b, s)/ b = (lfee(s+b) - lfee(b))/b`, is the derivative of `lfee`, ie `fee_rate(s) = lfee'(s)`.

Properties that we're looking for:
* validators should be able to pick parameters such that total bucket list size can grow to size `M_base` (that is deemed manageable by the ecosystem), but puts up a lot of resistance to grow to size `M_base+M_buffer` and beyond.
* `fee_rate(s)` should provide enough feedback for users and use cases to self-correct. It would not be desirable at the extreme to have very low fees up to `M_base` and suddenly "hit a wall" where fees shoot up to extremely high numbers after that.

Given those, the choice for `fee_rate` is constructed as the superposition of the following 2 functions (integrating yields the respective `lfee` component):
* `(feeRateM - feeRate)*s/M_base + feeRate` --> `(feeRateM - feeRate)*s^2/(2*M_base) + feeRate*s`
* `if s > M_base, exp(K*(s-M_base)/B_buffer)` --> `exp(K*(s-M_base)/B_buffer)*B_buffer/K`

Where `feeRate` and `feeRateM` are the fee rate at size 0 and `M_base` respectively.

Which together yields:

`lfee(s) = (feeRateM - feeRate)*s^2/(2*M_base) + feeRate*s + (if s > M_base, exp(K*(s-M_base)/B_buffer), 0)`.

With `K` picked such that `fee(1, M_base+M_buffer)` is orders of magnitude larger than what the market would be willing to pay.

We simplify those functions further by charging fees linearly to the number of bytes within a ledger (see rationale below).

As a consequence the final formula looks like this:

`fee(b) = round_up(b*fee_rate(s))`

With
`fee_rate(s) = (feeRateM - feeRate)*s/M_base + feeRate + (if s > M_base, exp(K*(s-M_base)/B_buffer), 0)`

We can simplify this even further by replacing the exponential component by a steep linear slope that causes fees to be "extremely high" at `M_buffer`.

##### Putting it together

"read/write" operations need to first read data before writing it. The amount of data written back can be larger or smaller than what was read, as consequence:
* The number of ledger entry reads is the size of ledger entries referenced in ledger footprints (both read and read/write).
* The number of bytes to read is the size of bucket entries from both the read and read/write footprints.
* The number of bytes to write is the number of bytes associated with bucket entries referenced by the readWrite footprint.
* The number of ledger entry to write is the size of the read/write footprint.

##### Effective fee and flooding

Having the fee model depend on ledger size creates some complication when trying to reason about multiple transactions getting applied: a naive solution would just try to follow the price curve exactly, causing fees to evolve on a per transaction basis.
This would create incentives to front-run transactions within the same transaction set, and would also make it hard to decide if a transaction should get flooded.

In the context of this proposal, we make the following observation: assuming that validators pick a small upper bound for the total number of bytes that can be added to the bucket list per ledger relative to the bucket list size, we can then assume that the price of storage varies marginally per ledger (ie the price paid by the first transaction is not that different from the last transaction in a ledger), and that the reference price still "resets" to the proper price every ledger (as it's based on the size of the bucket list).

As a consequence, we can just price transactions independently, based on the bucket list from the last closed ledger, and allocation within a transaction (for multiple bytes) is a flat rate as well.

Flooding transactions for the next ledger in that context is straightforward as the only factor to take into account is bucket list size (determined based on the last closed ledger).

Note that transactions can still be invalidated more than a ledger in the future, as a consequence validators may apply a certain level of "padding" when computing the fee required before flooding those transactions.

##### Ledger size reduction

So far we've established a model for deriving fees based on the bucket list size, but there needs to be a mechanism to ensure that the cost of storage does not grow indefinitely, hurting usability of the network.

Core ideas and principles:
* Ledger space is a shared public resource, policies should be set to ensure fair use.
* cost of using ledger space should converge towards market rate over time
   * in particular creating spam ledger entries should cost market rate over the long term.
* abandoned entries should not cost anything to network participants over the long term.

This proposal therefore depends on a solution with the following high level properties:
* ledger entries have to periodically pay for "rent", where the rent amount is adjusted on a per period basis (as to approximate "market rate")
* ledger entries that do not want to pay for rent anymore should be purged from the ledger, freeing up space for other entries (and lowering the overall price of storage)
    * purged entries may be recoverable by relying on external recovery nodes that can reconstruct proofs that validators can verify.

#### Historical storage

Historical storage corresponds to data that needs to be persisted by full validators outside of the bucket list.

This includes transactions and their result.

As the data is stored only once but for "eternity", it has to be priced accordingly (at a minimum, this data has to be made available as to allow validators to catch up to the network).

The model retained in the context of this CAP is to just have the validators set a flat rate per byte for this kind of data (updated on a regular basis as to track cost of storage over time).

##### Transaction Result

In order to reduce the base cost of transactions, the "result" published to archive is fixed size and the actual detailed transaction result is now emitted in the meta. See [CAP-0046: Smart Contract Events](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-08.md) for more details.

#### Extended meta data

Extended meta data here refers to parts of the meta data (produced when closing ledgers) that are not related to ledger changes:
* Smart contracts generate "events"
* `TransactionResult`

See [CAP-0046: Smart Contract Events](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-08.md) for more details.

Fees are needed to control for the overhead in those systems.

The model retained in this CAP is a flat rate per byte model for simplicity. It is expected that this fee would be orders of magnitude smaller than what is needed to persist data on chain.


#### Bandwidth

Transactions need to be propagated to peers on the network.

At the networking layer, transactions compete for bandwidth on a per ledger basis (`ledgerMaxPropagateSizeBytes`).

Note that validators may apply additional market dynamics due to implementation constraints, especially when trying to balance propagating large transactions vs smaller ones. See [CAP-0042: Multi-Part Transaction Sets](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0042.md).

##### Ephemeral payload

In the future, it may be possible to attach a `ephemeralPayload` (Hash + size), that gets cleared before applying transactions (used in the context of proofs of availability).

Further reading: [blob transactions in Ethereum](https://notes.ethereum.org/@vbuterin/blob_transactions).

#### Refunds on “flat rate” resources

Some resources are priced determined on a per ledger basis, independently of transaction set composition.

For such resources, a transaction gets charged the “worst case” utilization at the beginning of the transaction execution, and gets refunded based on actual usage at the end of the execution.

#### No refund for unused capacity on market based resources

If a transaction declares that it wants to use up to X units of a given resource, nominators assemble a transaction set with that information, potentially excluding other transactions because of this.

As a consequence, there should not be any refund for unused capacity. Specifically, if a resource was priced at a given rate by validators, the fee charged will be for the entire capacity (note that this still lets validators provide discounts on the rate).

#### Transaction fees and prioritization

This proposal assumes that fees charged for resources based on network settings are "fair", and that market dynamics should be shifted towards the "intent" of any given transaction (also called "social value" of a transaction).

This implies that:
* transactions are flooded/included purely based on their social value.
* additional throttling at the overlay layer may occur when some resources are scarce (similar to how in classic, the rate of operations that can be flooded is capped).

Note that the inclusion fee is *not* related to the amount of work that a transaction does. In other words, a transaction performing twice as much work than another but with the same inclusion fee bid are considered to have the same priority.

This simplification allows to remove entirely the need to model on chain a "synthetic universal resource" that can be used to represent the amount of work a given transaction performs (such as "gas" in Ethereum for example).

The following notable properties are expected with this model:
* adjustment to fee rates can be done using arbitrary models based on historical data, outside of the network
* in the future, additional logic can be added to have some price adjustment based on historical usage (similar to what is done for ledger space)
* validators (via CAP-0042 components) can still group similar transactions together.

#### Alternate fee model considered: multidimensional and uniform fees

Another way considered at some point was to try to dynamically price resources as to attain some sort of market rate as quickly as possible. This section goes over the approaches to implement "resource markets".

Note that we’re excluding “flat rate” resources where there is no competition from this section.

There are two ways to do it:
* have a separate market for each dimension. Transactions need to explicitly bid on each dimension.
   * This allows accurate price discovery for all resources. For example, if there is a lot of contention on "Instructions", this allows to discover the price of an instruction.
   * Relative priority between transactions is flexible, this is good (more room for innovation by nominators) and bad (harder for clients to know what to do to “get ahead”).
* transactions just specify a "fee bid" for multiple dimensions at once (potentially all markets at once)
   * there needs to be a function that computes the "minimum fee" for a given transaction, mixing all dimensions somehow (polynomial of sorts for example). Effectively creating a "synthetic universal resource".
   * comparing transactions can be done by comparing the ratio between the fee bid and the minimum fee, which is simple.
   * There is no price discovery of individual dimensions as people automatically bid more on all dimensions at once. That said, nominators can just pick "market prices" for each dimension that fits recent network conditions.

Both solutions require nominators to price resources (in much the same way that CAP-0042 allows nominators to price operations in the classic protocol).

The bidding is more complicated with the first approach. In order to come up with a reasonable bid, clients need not only to have 'market prices' for every resource, but also need to take into account the comparison algorithm used during transaction set building. For example, validators may consider ordering transactions by a tuple of bid-to-min-fee ratios for every resource (e.g. (instructions, IO, bandwidth)) and in order to prevent abuse of the fixed order, they would dynamically come up with that order depending on the current contents of the transaction queue. It's not obvious how to bid optimally for such an algorithm, as every ledger priorities might change several times.

For the second approach the bidding is comparable with the classic transactions: there is just a single 'market rate' for the smart contract transactions, that can be both used as a part of the bidding strategy and for comparison. The downside is that it requires maintaining parameters used to give different weights to the various resources as to come up with a "synthetic universal resource" that the network can reason about.

Related work:
  * Ethereum [Multidimensional EIP-1559](https://ethresear.ch/t/multidimensional-eip-1559/11651).

## Protocol Upgrade Transition
None, this fee model will only apply to smart contract transactions.

A subsequent CAP may update the fee model for the existing classic transaction subsystem as to be more consistent with this CAP.

### Resource Utilization

## Security Concerns


## Test Cases
## Implementation



