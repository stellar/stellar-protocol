```
CAP: 0046-07 (formerly 0055)
Title: Fee model in smart contracts
Working Group:
    Owner: MonsieurNicolas
    Authors: dmkozh
    Consulted:
Status: Draft
Created: 2022-06-03
Discussion: TBD
Protocol version: TBD
```

## Simple Summary

This CAP defines the mechanism used to determine fees when using smart contracts on the Stellar network.

## Working Group

TBD

## Motivation

With the introduction of smart contracts on the network, the existing fee model of the "classic" transaction system is too simplistic: it requires careful design of the code that runs "on chain" as to ensure that all operations have a similar cost and performance profile, which is not possible with arbitrary code running in contracts.

### Goals Alignment

Goals of the updated fee model are to:
* ensure fairness between users and use cases.
* promote scalable patterns on the network, doing more with the same amount of overall resources.
* ensure that the network operates in a sustainable way, network operators should be in control of their operating cost.

## Abstract

This CAP proposes various network level parameters (voted on by validators), and fee structure for the different kinds of resources involved on the network.

The fee structure is designed to discourage "spam" traffic and overall waste of infrastructure capacity.

## Specification

### XDR changes

The following network parameters are introduced:

```
// general “smart contract execution lane” settings
struct ContractExecutionLanesSettingsV0
{
    uint32 ledgerMaxTxCount; // maximum number of “smart” transactions per ledger
};

// "gas" aka "compute" settings
struct ContractGasNetworkSettingsV0
{
    int64 ledgerMaxGas; // maximum gas per ledger
    int64 txMaxGas; // maximum gas per transaction
    int64 minFeeRatePerGasIncrement; // minimum cost of GAS_INCREMENT=10k gas
};

// Ledger access settings
struct ContractLedgerCostNetworkSettingsV0
{
    uint32 ledgerMaxReadLedgerEntries;// maximum number of ledger entry read operations per ledger
    int64 ledgerMaxReadBytes; // maximum number of bytes that can be read per ledger
    uint32 ledgerMaxWriteLedgerEntries;// maximum number of ledger entry write operations per ledger
    int64 ledgerMaxWriteBytes; // maximum number of bytes that can be written per ledger

    uint32 txMaxReadLedgerEntries;// maximum number of ledger entry read operations per transaction
    int64 txMaxReadBytes; // maximum number of bytes that can be read per transaction
    uint32 txMaxWriteLedgerEntries;// maximum number of ledger entry write operations per transaction
    int64 txMaxWriteBytes; // maximum number of bytes that can be written per transaction


    int64 minFeeReadLedgerEntry; // minimum fee per ledger entry read
    int64 minFeeWriteLedgerEntry; // minimum fee per ledger entry write

    int64 minFeeRead1KB; // minimum fee for reading 1KB
    int64 minFeeWrite1KB; // minimum fee for writing 1KB

    int64 bucketListSizeBytes; // bucket list fees grow slowly up to that size
    int64 bucketListFeeRateLow; // fee rate in stroops when the bucket list is empty
    int64 bucketListFeeRateHigh; // fee rate in stroops when the bucket list reached bucketListSizeBytes
    int64 bucketListGrowthFactor; // rate multiplier for any additional data passed the first bucketListSizeBytes
};

// historical data settings
struct ContractHistoricalNetworkSettingsV0
{
    int64 txMaxResultSizeBytes; // maximum size of a transaction result
    int64 minFeeHistorical1KB; // minimum fee for storing 1KB in archives
};

// Extended data settings
struct ContractExtendedDataNetworkSettingsV0
{
    uint32 txMaxExtendedDataSizeBytes; // maximum size of extended data produced by a transaction
    int64 minFeeExtendedData1KB; // minimum fee for generating 1KB of extended data
};

// Network data settings
struct ContractNetworkDataNetworkSettingsV0
{
    int64 minFeeNetworkData1KB; // minimum fee for propagating 1KB of data
};

```

Additional changes at the Transaction/TransactionSet level:

```
// Transaction changes (actual diff TBD)

// int64 fee gets replaced by
   int64 gasFeeBid;
   int64 ledgerDataAccessFeeBid;
   int64 flatResourcesFee;

// Additional properties:
   uint32 gas;

   uint32 readBytes;
   uint32 writeBytes;

   uint32 resultSizeBytes; // probably removed (see below)

   uint32 extendedDataSize;
   
// Additional property to support ephemeral payloads
    EphemeralPayload* ephemeralPayload;

struct EphemeralPayload
{
     Hash hash;
     uint32 size;
};

// TransactionEnvelope changes
    opaque<> ephemeralPayload; // used in conjunction with tx.ephemeralPayload

// SCP value: hash of GeneralizedTransactionSet before and after removing payload


// GeneralizedTransactionSet changes TBD, need a way to express different fee markets
```

Changes to ledger data:
```

TBD: `LedgerEntry` with expiration date

```

### Semantics

Validity constraints:
* source account must be able to pay for the total fee bid for that transaction.
* the number of smart contract transactions cannot exceed `ledgerMaxTxCount` per ledger

#### Fee computation while applying transactions

As in classic, total fees are taken from the source account balance before applying transactions. Note that this is the observable behavior, implementations may decide to better stage updates.

At the end of the transaction execution, refund the source account for resources that are eligible for a refund (this refund is reflected under `txChangesAfter` in the meta).

Right now the following resources fall under this category:
  * historical data
  * extended data
  * network data

Total fee is equal to:
total_fee =  gasFeeBid + ledgerDataAccessFeeBid + flatResourcesFee;

__Open:__ it might be easier to make `tx.fee` the total fee, and derive `flatResourcesFee` from it, ie
`flatResourcesFee = total_fee - ( gasFeeBid + ledgerDataAccessFeeBid )`.

#### Execution time

A transaction contains:
* how much "gas" they want to bid for. `uint32 Tx.gas`.
* the fee bid they're willing to pay for that capacity. `int64 Tx.gasFeeBid`.

`min_gas_fee(Tx) = round_up(Tx.gas*minFeeRatePerGasIncrement/GAS_INCREMENT)`

Validity constraints:
* per transaction
    * `Tx.gas <= txMaxGas`.
    * `Tx.gasFeeBid >= min_gas_fee(Tx)`.
* ledger wide (`GeneralizedTransactionSet`)
  * sum of all `tx.gas` <= `ledgerMaxGas`.

#### Ledger data

A transaction contains:
* the read `tx.LedgerFootprint.readOnly` and read/write `tx.LedgerFootprint.readWrite` sets (ledger keys).
* the maximum total amount of data `uint32 tx.readBytes` that gets read by `tx.LedgerFootprint.readOnly` and `tx.LedgerFootprint.readWrite`.
* the maximum total amount of data `uint32 tx.writeBytes` that can be written by `tx.LedgerFootprint.readWrite`.
* the fee bid for ledger data (both reads and writes) `int64 tx.ledgerDataAccessFeeBid`.

```
min_LedgerData_fee(tx) =
     (length(tx.LedgerFootprint.readOnly)+length(tx.LedgerFootprint.readWrite))*minFeeReadLedgerEntry + // cost of reading ledger entries
  round_up(tx.readBytes * minFeeRead1KB / 1024) + // cost of processing reads
  length(tx.LedgerFootprint.readWrite)*minFeeWriteLedgerEntry + // cost of writing ledger entries
  round_up(wfee_rate(lcl.BucketListSize)* tx.writeBytes)) // cost of adding to the bucket list
```

With

```
wfee_rate(s) = (bucketListFeeRateHigh - bucketListFeeRateLow)*s/bucketListSizeBytes +
bucketListFeeRateLow +
(if s > bucketListSizeBytes,
    bucketListGrowthFactor*
    (bucketListFeeRateHigh - bucketListFeeRateLow)*
    (s-bucketListSizeBytes)/bucketListSizeBytes,
    0)
```

__Open:__ `wfee_rate` (and possibly `min_LedgerData_fee`) needs to be reconciled with "rent" (not finalized at the time of this writing) as "paying rent" is very similar to adding an entry to the ledger. The difference is that adding an entry to the ledger adds the entry to the "topmost" bucket, where as paying rent is adding an entry to a bucket while merging.

Validity constraints:
* per transaction
   * `length(tx.LedgerFootprint.readOnly) <= txMaxReadLedgerEntries`.
   * `tx.readBytes <= txMaxReadBytes`.
   * `length(tx.LedgerFootprint.readWrite) <= txMaxWriteLedgerEntries`.
   * `tx.writeBytes <= txMaxWriteBytes`.
* ledger wide (`GeneralizedTransactionSet`)
   * `sum(length(tx.LedgerFootprint.readOnly) + length(tx.LedgerFootprint.readWrite)) <= ledgerMaxReadLedgerEntries`.

#### Historical storage

A transaction contains:
* `uint32 tx.resultSizeBytes` the number of bytes for the result.
* `int64 tx.flatResourcesFee` fee shared by all “flat fee” resources

`min_historical_fee(txEnvelope) = round_up((size(txEnvelopeNoPayload)+txEnvelope.tx.resultSizeBytes) * minFeeHistorical1KB / 1024)`

Where `txEnvelopeNoPayload` is the envelope with the payload cleared out.

Validity constraints:
* `txEnvelope.tx.resultSizeBytes <= txMaxResultSizeBytes`.

#### Extended data

A transaction contains:
* `uint32 tx.extendedDataSize` the maximum size of extended data produced by this transaction
* `int64 tx.flatResourcesFee` fee shared by all “flat fee” resources

`min_ExtendedData_fee(tx) = round_up(tx.extendedDataSize * minFeeExtendedData1KB / 1024)`

Validity constraints:
* `tx.extendedDataSize <= txMaxExtendedDataSizeBytes`

#### Network data

A transaction contains:
* `int64 tx.flatResourcesFee` fee shared by all “flat fee” resources

`min_NetworkData_fee(tx) = round_up(size(txEnvelope) * minFeeNetworkData1KB / 1024)`

Validity constraints:
* `size(txEnvelope) <= txMaxTxSizeBytes`

## Design Rationale

### Fee estimation

This proposal relies heavily on the existence of a "preflight" mechanism to determine all parameters needed to compute minimum fees.

Additional logic (not covered in this CAP), will be needed to determine the market rate of resources based for example on historical data (see below).


### Resources

Fees are used to ensure fair and balanced utilization of resources.

For each resource type, we're assuming a model where we can define:
* the maximum resource consumption for a transaction, as to protect the network.
* a minimum "floor" price for a given transaction, as to ensure that there are no broken markets
* additional constraints may include
    * a "ledger wide" maximum as to protect the network and downstream systems when producing blocks.
    * "execution lane" maximum, as to ensure that execution lanes (executed in parallel), are balanced. This CAP does not attempt to define actual semantics or fee models related to parallel execution, and is mentioned here for context.

We’re also assuming that resource allocation is done independently of “classic” transactions (ie: the amount of resources allocated to smart contract execution is independent of other traffic). This points to “smart contract transactions” being managed as their own “phase” (in `GeneralizedTransactionSet` terminology) and having its own dedicated capacity expressed in terms of transactions (`ledgerMaxTxCount`).

The only reason for having "minimum fees" (on top of "on chain market dynamics"), is to combat "spam" transactions and ensure that there is no strange incentive to perform certain operations on chain instead of performing them on other systems with worse properties (like centralized cloud infrastructure).

Validators are expected to vote regularly (once a quarter for example) to ensure that fees are set correctly for the broader ecosystem. The exact way minimum fees are established is outside the scope of this document.

#### Compute

[CAP-0046: WebAssembly Smart Contract Runtime Environment](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-01.md) introduces the notion of "gas". In the context of this CAP, the only thing that matters is that "gas" represents an arbitrary base unit for "execution time".

As a consequence, the "goal" for validators is to construct a `GeneralizedTransactionSet` that uses up to `lcl.ContractNetworkSettingsV0.ledgerMaxGas`.

#### Ledger data

##### Read traffic

Reads are logically performed *before* transaction execution.

When performing reads of a ledger entry:
* The ledger entry needs to be located via some index in the ledger and the entry loaded. Depending on the underlying database technology, this translates to at least 1 disk operation.
* The bucket entry needs to be xdr decoded.

The resources to allocate in this context are therefore:
* a maximum number of ledger entry read operations in a ledger `ledgerMaxReadLedgerEntries`.
* a maximum number of bytes that can be read in a ledger `ledgerMaxReadBytes`.

The cost of a "ledger entry read" is fairly open ended, and depends on many variables. In this proposal, we give it a "base cost" for simplicity even if it translates to multiple disk operations (which is typically the case when using B-Trees for example, or if the ledger entry is retrieved by lookup over multiple buckets).

That "base cost" is defined by validators as `minFeeReadLedgerEntry`. This proposal does not let transactions compete directly on the number of ledger entry read operations, therefore the cost of a read operation is `minFeeReadLedgerEntry` (validators must still construct transaction sets that keep the number of reads below a maximum).

Market dynamics are limited to the number of bytes (see below).

Transactions contain the total number of bytes that they will read from the bucket list as well at a fee bid for reading those bytes.

The number of bytes read corresponds to the size of the latest `BucketEntry` for that ledger entry (and does not take into account the possibility that an implementation may read stale entries in buckets or may have to read other entries from a bucket).

The minimum fee bid is determined based on the rate `minFeeRead1KB` expressed for reading 1 KB (1024 bytes) worth of data.

As transactions compete for the total read capacity `ledgerMaxReadBytes` for a given ledger, the effective fee goes up.


##### Write traffic and ledger size

Writes are performed *after* transaction execution, and are blocking the actual closing of a ledger.

When writing a ledger entry:
* The bucket entry is marshaled to binary.
* The bucket entry is appended to the topmost bucket serially.
* The bucket entry is read, hashed and written back with every level merge operation.

In this proposal, we're modeling "worst case": a bucket entry gets added to the bucket list and has to travel all the way to the bottom bucket, contributing as many bytes as the bucket entry itself.

In that case, the overhead is dominated by the size of buckets and bucket entries, and the number of bucket entries is not really a factor when merging.

Consequently, we can model the cost of a write as an append to the overall bucket list and charge a "base rate" for adding a bucket entry.

For allocating ledger entry writes, the model is analogous to "reads": a ledger is constructed as to not exceed `ledgerMaxWriteLedgerEntry` writes and each write contributes `minFeeWriteLedgerEntry` to the overall fee for that transaction (no market dynamics here).

As for "bytes written", the model that was chosen is:
* use the total bucket list size as the main resource to track.
* a cost function, allows to price the cost of expanding ledger size.
* ledger size, and therefore price of storage, goes down as bucket entries get merged/deleted.

The cost function that was selected is similar to what was proposed in Ethereum's [make EIP 1559 more like an AMM curve](https://ethresear.ch/t/make-eip-1559-more-like-an-amm-curve/9082).

The main point being that the fee for adding `b` bytes to a bucket list of size `s` is calculated as `fee(b,s) = lfee(s + b) - lfee(s)`, where `lfee` is the "total cost to build a bucket list of a given size".
When designing for specific properties of that function, it's useful to see the "fee rate": `fee_rate(s) = lim b->0, fee(b, s)/ b = (lfee(s+b) - lfee(b))/b`, is the derivative of `lfee`, ie `fee_rate(s) = lfee'(s)`.

Properties that we're looking for:
* validators should be able to pick parameters such that total bucket list size can grow to size `M_base` (that is deemed manageable by the ecosystem), but puts up a lot of resistance to grow to size `M_base+M_buffer` and beyond.
* `fee_rate(s)` should provide enough feedback for users and use cases to self-correct. It would not be desirable at the extreme to have very low fees up to `M_base` and suddenly "hit a wall" where fees shoot up to extremely high numbers after that.

Given those, the choice for `fee_rate` is constructed as the superposition of the following 2 functions (integrating yields the respective `lfee` component):
* `(feeRateM - minFeeRate)*s/M_base + minFeeRate` --> `(feeRateM - minFeeRate)*s^2/(2*M_base) + minFeeRate*s`
* `if s > M_base, exp(K*(s-M_base)/B_buffer)` --> `exp(K*(s-M_base)/B_buffer)*B_buffer/K`

Where `minFeeRate` and `feeRateM` are the fee rate at size 0 and `M_base` respectively.

Which together yields:

`lfee(s) = (feeRateM - minFeeRate)*s^2/(2*M_base) + minFeeRate*s + (if s > M_base, exp(K*(s-M_base)/B_buffer), 0)`.

With `K` picked such that `fee(1, M_base+M_buffer)` is orders of magnitude larger than what the market would be willing to pay.

We simplify those functions further by charging fees linearly to the number of bytes within a ledger (see rationale below).

As a consequence the final formula looks like this:

`fee(b) = round_up(b*fee_rate(s))`

With
`fee_rate(s) = (feeRateM - minFeeRate)*s/M_base + minFeeRate + (if s > M_base, exp(K*(s-M_base)/B_buffer), 0)`

We can simplify this even further by replacing the exponential component by a steep linear slope that causes fees to be "extremely high" at `M_buffer`.

##### Putting it together

"read/write" operations need to first read data before writing it. The amount of data written back can be larger or smaller than what was read, as consequence:
* The number of ledger entry reads is the size of ledger entries referenced in ledger footprints (both read and read/write).
* The number of bytes to read is the size of bucket entries from both the read and read/write footprints.
* The number of bytes to write is the number of bytes associated with bucket entries referenced by the readWrite footprint.
* The number of ledger entry to write is the size of the read/write footprint.

##### Effective fee and flooding

Having the fee model depend on ledger size creates some complication when trying to reason about multiple transactions getting applied: a naive solution would just try to follow the price curve exactly, causing fees to evolve on a per transaction basis.
This would create incentives to front-run transactions within the same transaction set, and would also make it hard to decide if a transaction should get flooded.

In the context of this proposal, we make the following observation: assuming that validators pick a small upper bound for the total number of bytes that can be added to the bucket list per ledger relative to the bucket list size, we can then assume that the price of storage varies marginally per ledger (ie the price paid by the first transaction is not that different from the last transaction in a ledger), and that the reference price still "resets" to the proper price every ledger (as it's based on the size of the bucket list).

As a consequence, we can just price transactions independently, based on the bucket list from the last closed ledger, and allocation within a transaction (for multiple bytes) is a flat rate as well.

Flooding transactions for the next ledger in that context is straightforward as the only factor to take into account is bucket list size (determined based on the last closed ledger).

##### Ledger size reduction

So far we've established a model for deriving fees based on the bucket list size, but there needs to be a mechanism to ensure that the cost of storage does not grow indefinitely, hurting usability of the network.

Core ideas and principles:
* Ledger space is a shared public resource, policies should be set to ensure fair use.
* cost of using ledger space should converge towards market rate over time
   * in particular creating spam ledger entries should cost market rate over the long term.
* abandoned entries should not cost anything to network participants over the long term.

This proposal therefore depends on a solution with the following high level properties:
* ledger entries have to periodically pay for "rent", where the rent amount is adjusted on a per period basis (as to approximate "market rate")
* ledger entries that do not want to pay for rent anymore should be purged from the ledger, freeing up space for other entries (and lowering the overall price of storage)
    * purged entries may be recoverable by relying on external recovery nodes that can reconstruct proofs that validators can verify.

#### Historical storage

Historical storage corresponds to data that needs to be persisted by full validators outside of the bucket list.

This includes transactions and their result.

As the data is stored only once but for "eternity", it has to be priced accordingly (at a minimum, this data has to be made available as to allow validators to catch up to the network).

The model retained in the context of this CAP is to just have the validators set a flat rate per byte for this kind of data (updated on a regular basis as to track cost of storage over time).

##### Future work

__Open:__ we should consider doing this now as we get rid of the "result size" attached to the transaction, this is being discussed as part of “events”.

In order to reduce the base cost of transactions, the "result" could be replaced by just a hash (potentially the root of a Merkle tree that contains the result and meta information) that refers to extended data: the result is not necessary in the context of a replay, but is useful when trying to verify from a light client.


#### Extended data

__Open:__ this resource would benefit from refunds (as there is no "per ledger" limit).

Extended data here refers to the "meta" generated when closing ledgers.

Smart contracts generate "events" that need to be processed by downstream systems (like Horizon).

__Open:__ should this fee exclude ledger changes (already covered by other fees).

Fees are needed to control for the overhead in those systems.

The model retained in this CAP is a flat rate per byte model for simplicity. It is expected that this fee would be orders of magnitude smaller than what is needed to persist data on chain.


#### Bandwidth

Transactions need to be propagated to peers on the network.

Fees to propagate transactions as they are now can just be included in the "Historical storage" cost (that already charges for the cost of `TransactionEnvelope`).

In order to support payloads that get discarded by validators, bandwidth needs to be tracked separately.

Transaction contains an optional `payload` (Hash + size), and the `TransactionEnvelope` gets the actual payload, that gets cleared before applying transactions.

The model retained in this CAP is a flat rate per byte model for simplicity. It is expected that this fee would be orders of magnitude smaller than what is needed to persist data on chain.

#### Base rates are independent of transaction ordering

The "base rate" for resources is picked to be the same for all transactions in a given fee group (see CAP-0042 for definition on “groups”) within a ledger.

This avoids creating incentives for "front running" transactions within a ledger just to get a better rate.

#### Refunds on “flat rate” resources

Some resources are priced determined on a per ledger basis, independently of transaction set composition.

For such resources, a transaction gets charged the “worst case” utilization at the beginning of the transaction execution, and gets refunded based on actual usage at the end of the execution.

#### No refund for unused capacity on market based resources

If a transaction declares that it wants to use up to X units of a given resource, nominators assemble a transaction set with that information, potentially excluding other transactions because of this.

As a consequence, there should not be any refund for unused capacity. Specifically, if a resource was priced at a given rate by validators, the fee charged will be for the entire capacity (note that this still lets validators provide discounts on the rate).

#### Multidimensional vs uniform fees


__Open:__ this choice depends on how many markets we really have. This proposal has 2 dimensions: gas, number of bytes read/written. We could instead predefine markets and let market price be determined in each of those markets, this would allow to reduce the number of “bids” to a single value. For example, there could be the following markets: <compute, low read and write>, <low compute, high read and write>. As the “on the wire” savings are not that high (8 bytes saving), we’ve decided to leave bids separate at this time.

As transactions are competing for some resources, there needs to be a way to prioritize transactions between each other.

Note that we’re excluding “flat rate” resources where there is no competition from this section.

There are two ways to do it:
* have a separate market for each dimension. Transactions need to explicitly bid on each dimension.
   * This allows accurate price discovery for all resources. For example, if there is a lot of contention on "gas", this allows to discover the price of a unit of gas.
   * Relative priority between transactions is flexible, this is good (more room for innovation by nominators) and bad (harder for clients to know what to do to “get ahead”).
* transactions just specify a "fee bid" for multiple dimensions at once (potentially all markets at once)
   * there needs to be a function that computes the "minimum fee" for a given transaction, mixing all dimensions somehow (polynomial of sorts for example).
   * comparing transactions can be done by comparing the ratio between the fee bid and the minimum fee, which is simple.
   * There is no price discovery of individual dimensions as people automatically bid more on all dimensions at once. That said, nominators can just pick "market prices" for each dimension that fits recent network conditions.

Both solutions require nominators to price resources (in much the same way that CAP-0042 allows nominators to price operations in the classic protocol).

For clients to compute accurate fee bids, both versions need access to "market prices" for individual resources. As a consequence, there is really no advantage to the "uniform fee" model other than transaction size.

Related work:
  * Ethereum [Multidimensional EIP-1559](https://ethresear.ch/t/multidimensional-eip-1559/11651).


## Protocol Upgrade Transition
None, this fee model will only apply to smart contract transactions.

A subsequent CAP may update the fee model for the existing classic transaction subsystem as to be more consistent with this CAP.

### Resource Utilization

## Security Concerns


## Test Cases
## Implementation



