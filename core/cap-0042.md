## Preamble

```
CAP: 0042
Title: Multi-Part Transaction Sets
Working Group:
    Owner: nicolas@stellar.org
    Authors: TBD
    Consulted: TBD
Status: Draft
Created: 2021-11-29
Discussion: TBD
Protocol version: TBD
```

## Simple Summary

This CAP adds additional structure to transaction sets such that transaction sets can be expressed in terms of groups of transaction sets as to faciliate implementing per group policies such as order book aware surge pricing.

## Working Group

TBD

## Motivation

Right now, there is no mechanism that makes it possible to implement different fee structures based on transaction set composition; as a consequence, transaction set composition can sometimes be heavily biased towards undesirable behaviors such as spikes of arbitrage trades that eventually fail.

This CAP aims to correct this by isolating subsets of transactions that can compete on fees without causing overall fees to increase. At the same time, it makes it possible to expand policies to other dimensions in the future.

### Goals Alignment

The Stellar network aims to give equitable access to the global financial infrastructure, in particular the network should not benefit a minority of participants at the expense of others.

This CAP aims at restoring balance between use cases on the network.

## Abstract

Right now transaction fees are computed for a given ledger based on [CAP-0005 Throttling and transaction pricing improvements](CAP-0005.md).

CAP-005 defines:
* How a transaction set (to be applied for the next ledger) gets constructed from an arbitrary number of candidate transactions as to enforce the policy that puts a bound on the number of operations present in any given ledger.
* The base fee to use when applying a transaction set

This CAP generalizes the above in the following way:
* It allows to describe transaction sets in terms of sets of transaction subsets
* Each subset gets constructed in a similar way to CAP-005, following additional policies
* Each subset gets its own base fee

Note that this CAP does not modify how transactions gets applied (ie: the apply order) other than fee computation.

## Specification

### XDR Changes

```diff
diff --git a/src/xdr/Stellar-ledger.x b/src/xdr/Stellar-ledger.x
index 84b84cbf7..092fb9e22 100644
--- a/src/xdr/Stellar-ledger.x
+++ b/src/xdr/Stellar-ledger.x
@@ -176,14 +176,52 @@ case METAENTRY:
     BucketMetadata metaEntry;
 };

-// Transaction sets are the unit used by SCP to decide on transitions
-// between ledgers
+enum TransactionSubsetType
+{
+  TSUBSET_SURGE = 1
+};
+
+// Subsets are expressed in terms of indices
+// based on the main transaction set.
+// Indices must be in strictly ascending order.
+// a subset cannot be empty
+typedef int TxIndices<>;
+
+union TransactionSubset switch (TransactionSubsetType t)
+{
+case TSUBSET_SURGE:
+  struct
+  {
+    int64 baseFee;
+    TxIndices txIndices;
+  } txSubsetSurge;
+};
+
 struct TransactionSet
 {
     Hash previousLedgerHash;
     TransactionEnvelope txs<>;
 };
+
+struct TransactionSetV1
+{
+    Hash previousLedgerHash;
+    TransactionEnvelope txs<>; // sorted in hash order (like a `TransactionSet`)
+    TransactionSubSet subsets<>;
+};
+
+// Transaction sets are the unit used by SCP to decide on transitions
+// between ledgers
+
+union GeneralizedTransactionSet switch (int v)
+{
+case 0:
+    TransactionSet v0TxSet;
+case 1:
+    TransactionSetV1 v1TxSet;
+};
+
 struct TransactionResultPair
 {
     Hash transactionHash;
@@ -203,11 +241,13 @@ struct TransactionHistoryEntry
     uint32 ledgerSeq;
     TransactionSet txSet;

-    // reserved for future use
+    // when v != 0, txSet must be empty
     union switch (int v)
     {
     case 0:
         void;
+    case 1:
+        GeneralizedTransactionSet txGeneralizedSet;
     }
     ext;
 };
@@ -358,9 +398,30 @@ struct LedgerCloseMetaV0
     SCPHistoryEntry scpInfo<>;
 };

+struct LedgerCloseMetaV1
+{
+    LedgerHeaderHistoryEntry ledgerHeader;
+    // NB: txSet is sorted in "Hash order"
+    GeneralizedTransactionSet txSet;
+
+    // NB: transactions are sorted in apply order here
+    // fees for all transactions are processed first
+    // followed by applying transactions
+    TransactionResultMeta txProcessing<>;
+
+    // upgrades are applied last
+    UpgradeEntryMeta upgradesProcessing<>;
+
+    // other misc information attached to the ledger close
+    SCPHistoryEntry scpInfo<>;
+};
+
 union LedgerCloseMeta switch (int v)
 {
 case 0:
     LedgerCloseMetaV0 v0;
+case 0:
+    LedgerCloseMetaV1 v1;
 };
-}
+
+} // namespace stellar
```

### Semantics

#### Consensus value

SCP messages and ledger header reference transaction sets by hash. Consequently, the preimage supplied can transparently be migrated to `GeneralizedTransactionSet` when voting for a transaction set that can be applied using the version of the protocol with this CAP.

In summary: if `lcl.ledgerHeader.version` is less than `P` (version this CAP takes effect), the preimage must be a `TransactionSet`, otherwise it must be a `GeneralizedTransactionSet`.

#### Value Validation

A `v1TxSet` is validated using the following rules:
* `previousLedgerHash` is equal to the hash of the previous ledger
* `txs` is a valid transaction set (validated like a `TransactionSet.txs`)
* `subsets` do not overlap (ie: an index cannot be referenced by several subsets)

Each `TSUBSET_SURGE` subset is validated with:
* `txIndices` is not empty
* transactions within `txIndices`:
  * are sorted in strictly ascending order
  * must reference valid transactions from `txs` (ie, each index must be within bounds)
* `basefee` must be strictly greater than the effective base fee derived from `txs` following CAP-0005 rules.
* `basefee` must be smaller (or equal) than the smallest base fee bid from transactions included in that subset.

#### Effective fee computation

The effective fee for a given transaction is computed in the following way:
  * If the transaction is in a `TSUBSET_SURGE` subset, the base fee is defined by that subset's `basefee` (overriding the standard base fee).
  * Otherwise, the transaction is considered as part of the "default" subset. Its base fee is
    derived from the overall transaction set `txs` using the same logic than CAP-0005.

Note that this is not ambigiuous as a transaction can only be present in at most one subset by construction.

#### Candidate value generation

Just like today, this CAP does not specify the exact algorithm used to produce a valid value as to make it easier for implementations to compete on the quality of transaction sets.

A potential algorithm that could be used in the initial rollout of this CAP can be something like:

1. build a "surge pricing order iterator" (similar to CAP-0005), that feeds transactions in decreasing priority order
2. iterate over transactions in that order and:
   1. if the overall candidate set is within 101 operations of the maximum allowed, stop
   2. map each transaction to tuples `(queue of transactions={}, bool surge_pricing=false)`, that each correspond to possible orderbook pairs referenced by that transaction in the context of an arb loop:
      * Examples:
        * `tx1 = { PathPaymentStrictReceiveOp: {sendAsset: A, ..., destAsset:A, path: [B] }}` gets associated to `<A,B>` and `<B,A>`.
        * `tx2 = { PathPaymentStrictReceiveOp: {sendAsset: A, ..., destAsset:A, path: [B,C] }}` gets associated to `<A,B>`, `<B,C>` and `<C,A>` (the "path") and also `<A,C>` (as this payment converts an `A` into a `C`) and `<B,A>`.
        * `tx3 = { SetOptions ...}` gets mapped to the default queue.
    1. if the transaction can be inserted into all associated queues
       * without causing any queue to go over its operations limit `K` (picked as `max(101, ledgerHeader.maxTxSetSize/10)`)
       * and none of the queues have `surge_pricing==true`,
       * insert it in all those queues.
    2. otherwise
       * drop the transaction, and skip all remaining transactions from that source account (as subsequent sequence numbers would be invalid).
       * mark that queue with `surge_pricing=true`.
3. Iterate over all queues and merge queues that are not in surge pricing into the default queue.
4. Generate `txs`, sorted in "hash order"
5. Compact queues
    * Delete redundant or empty queues.
    * Simplify queues (remove duplicates) as to ensure that transactions are only referenced from one subset.
    * Merge surge queues with the same `basefee`.
6. Produce subset(s) that correspond to each queue.

#### Nomination value comparison

The order used by the nomination protocol is altered to deal with subsets in case of conflict.

Values are sorted in lexicographic order by:
* number of operations (as to favor network throughput) _unchanged_,
* total fees that the transaction set will collect (as to maximize quality of transaction sets) _unchanged_,
* number of subsets in descending order (to promote merging subsets),
* size of indices in descending order (to promote deduping),
* hash as a tie-breaker _unchanged_

#### Implications on transaction flooding strategy

Not technically part of the protocol but described here for completeness.

Today's implementation uses "surge pricing order" to prioritize which transaction to flood next. This does not need to change.

The change that can be done at the transaction queue level is to expand the policy as to support something similar to "step 2" above under [Candidate Value Generation](#candidate-value-generation) :
today's transaction queue policy only allows nodes to accumulate up to a maximum number operations in memory (by applying a multiplier on `ledgerHeader.maxTxSetSize`). The policy can be expanded to also track per order book counters, and not accept transactions that would cause to go over that limit (in this case, we would probably just the raw policy number and not even allow to accumulate more than what would be included in a ledger).

## Design Rationale

The proposal tries to limit to a minimum the number of things actually enforced at the protocol layer, leaving the door open to more flexibility at the node level.

In turn, this flexibility should allow for faster iteration without having to take a dependency on network wide protocol changes.

The choice of using "indices" to represent subsets (as opposed to embedding transactions directly), leaves some flexibility on the use of subsets in the future that "tag" transactions for purpose other than fee processing. Depending on those use cases, those new subset types may or may not overlap with subsets introduced in this CAP.

## Protocol Upgrade Transition

As soon as this CAP becomes active, nodes will produce new SCP values stored in the ledger header.

### Backwards Incompatibilities

This CAP does not change transaction semantics, impact on the ecosystem should be minimal.
Systems that try to compute transaction fees that are close to market rate will have to be adjusted as "ledger capacity" would only be losely related to "local surge pricing".
Users that just bid what they're willing to spend on fees, will not be effected.

### Resource Utilization

This change will require a small increase in CPU and memory utilization which should easily be recovered by the expected decrease in failed transactions.

## Security Concerns

The changes on the security front are minimal as transaction semantics are not changed.

This CAP gives more control to validators when it comes to fees: a validator could isolate an arbitrary subset of transactions as to force higher fees on them.

This should not be a problem in practice:
* transactions will never be charged more than their maximum bid, and people tend to put small multipliers on top of market rate (fees remain low).
* as validators do not receive fees from transactions processing, there is little incentive for a validator to do this.
* if validators target certain transactions like this, as values are signed by the validator that produced the value, the faulty validator will accrue negative reputation on the network.
 
## Future work

This CAP leaves the door open to how subsets can be used.

Here are a few examples:
* Subsets can be used to prioritize based on dimensions other than order book as to rebalance use cases, for example certain operations could be segregated some more.
* Subsets could be used to alter the "apply order" policy: some subsets may get applied before others, or (groups of) subsets could be applied in some interleaved way.
* Subsets could be applied using completely different logic. For example, some subsets may not allow trading at all, or enable different semantics.
* Subsets could also be a building block for "parallel execution": subsets could be organized into "independent units" (that can be applied in parallel in some way).

## Test Cases

TBD

## Implementation

TBD
