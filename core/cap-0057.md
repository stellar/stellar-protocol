```
CAP: 0057
Title: State Archival Persistent Entry Eviction
Working Group:
    Owner: Garand Tyson <@SirTyson>
    Authors: Garand Tyson <@SirTyson>
    Consulted: Dmytro Kozhevin <@dmkozh>, Nicolas Barry <@MonsieurNicolas>
Status: Draft
Created: 2024-06-24
Discussion: https://github.com/stellar/stellar-protocol/discussions/1480
Protocol version: 22
```

## Simple Summary

This proposal allows the network to evict `PERSISTENT` entries, i.e., delete archived `PERSISTENT` entries from validators.

## Working Group

As specified in the Preamble.

## Motivation

To lower the storage requirements of validators and decrease the growth of History Archives.

### Goals Alignment

This change is aligned with the goal of lowering the cost and increasing the scale of the network.

## Abstract

Whenever a `PERSISTENT` entry is evicted or deleted by a transaction (as in deleted as part of TX execution), it will be removed from
the “Live State BucketList” (today called the BucketList) and added to the “Archived State Tree” (AST). The AST is made up of a collection
of immutable Merkle trees stored on top of the BucketList, called “Archival Snapshots”, plus a single mutable store called the
“Hot Archive”.

At any given point, all validators store the Hot Archive on disk. Whenever a `PERSISTENT` entry is evicted, it is added to the
Hot Archive. Eventually, the Hot Archive becomes full. At this point, the Hot Archive is “snapshotted” and converted into an immutable
Archival Snapshot. Validators retain only the Merkle root of the newly created Archival Snapshot and delete the rest of the snapshot.
The validators then initialize a new, empty Hot Archive and repeat the process.

While validators only store the Merkle root of each Archival Snapshot, the complete Archival Snapshots are persisted in the History
Archive. RPC nodes may locally store as few or as many of these snapshots as the operator desires, initialized directly via History
Archive files (RPC providers may also shard snapshots across multiple nodes such that no one RPC node is required to store all
snapshots). During preflight, RPC will use these local snapshots to attach Merkle style proofs for any archived entry encountered during preflight. These proofs can then be submitted via the `RestoreFootprintOp` to allow the entries to be used again and become part of the
Live State BucketList.

## Specification

### XDR changes

```
enum BucketEntryType
{
    ...
    RESTORED = 3   // At-and-after protocol 22: only Hot Snapshot
};

union BucketEntry switch (BucketEntryType type)
{
case LIVEENTRY:
case INITENTRY:
    LedgerEntry liveEntry;

case DEADENTRY:
case RESTORED:
    LedgerKey deadEntry;
case METAENTRY:
    BucketMetadata metaEntry;
};

enum ArchivalBucketEntryType
{
    LIVELEAF = 0,
    DEADLEAF = 1,
    HASHENTRY = 2,
    BOUNDARY = 3
};

struct ArchivalHashEntry
{
    uint32 level
    Hash hash;
};

struct ArchivalBucketEntry
{
    uint32 index;

    switch (ArchivalBucketEntryType type)
    {
    case LIVELEAF:
        LedgerEntry liveLeaf;
    case DEADLEAF:
        LedgerKey deadLeaf;
    case HASHENTRY:
        ArchivalHashEntry hashEntry;
    case BOUNDARY:
        bool isLowerBound;
    }
    data;

    ExtensionPoint ext;
};

enum LedgerEntryType
{
    ...
    BLOB = 10 // Stores arbitrary data for validator operation
};

struct LedgerEntry
{
    uint32 lastModifiedLedgerSeq; // ledger the LedgerEntry was last changed

    union switch (LedgerEntryType type)
    {
        ...
        case BLOB:
            BlobEntry blob;
    }
    data;

    // reserved for future use
    union switch (int v)
    {
    case 0:
        void;
    case 1:
        LedgerEntryExtensionV1 v1;
    }
    ext;
};

struct ArchivalFilter
{
    uint64 epsilon;
    uint32 numHashes;
    opaque seed<16>;
    opaque filterBytes<>;
};

union LedgerKey switch (LedgerEntryType type)
{
...
case BLOB:
    struct
    {
        int64 blobID;
        BlobType type;
    } blob;
};

enum BlobType
{
    ARCHIVAL_MERKLE_ROOT = 0,
    ARCHIVAL_FILTER = 1
};

struct BlobEntry
{
    int64 blobID; // Blob ID is the archival epoch
    union switch (BlobType type)
    {
        case ARCHIVAL_MERKLE_ROOT:
            ArchivalHashEntry hash;
        case ARCHIVAL_FILTER:
            ArchivalFilter filter;
    } data;
};

enum ConfigSettingID
{
...
CONFIG_SETTING_STATE_ARCHIVAL_EXT = 14,
CONFIG_SETTING_STATE_ARCHIVAL_META = 15
};

// Contains state archival state that cannot be changed via
// config upgrade
struct StateArchivalMeta
{
    uint32 currArchivalEpoch;
    uint32 hotArchiveSize; // Current number of entries in the Hot Archive

    // Current iterator position for Merkle Generation Queue
    uint32 merkleQueueLevel;
    uint32 merkleQueueIndex;

    // Ledger at which the current pending archival snapshot will be
    // added to the merkle production queue
    uint32 pendingSnapshotPublishLedger;
};

union ConfigSettingEntry switch (ConfigSettingID configSettingID)
{
...
case CONFIG_SETTING_STATE_ARCHIVAL_EXT:
    StateArchivalSettingsExt stateArchivalSettingsExt;
case CONFIG_SETTING_ARCHIVAL_META:
    StateArchivalMeta stateArchivalMeta;
};

struct StateArchivalSettingsExt
{
// Target false positive rate for filter = 1 / epsilon
uint64 filterEpsilon;

// When calculating rent rates based on BucketList size, discount archival
// Bucket List sizes by numerator / denominator.
uint64 archivalBucketListSizeDiscountNumerator;
uint64 archivalBucketListSizeDiscountDenominator;

// Rate limit how fast cold snapshots are converted to Merkle trees
uint32 maxEntriesToHash;
uint32 maxBytesToHash;

// Number of entries at which archival snapshot is considered full
uint32 archivalSnapshotSize;

// Number of levels in Archival Snapshot BucketList
uint32 archivalSnapshotDepth;

// The number of ledger between which the Hot Archive becomes full
// and when the corresponding archival snapshot is initialized and
// added to Merkle Production Queue
uint32 numLedgersToInitSnapshot;
};

enum ArchivalProofType
{
    EXISTENCE = 0,
    NONEXISTENCE = 1
};

struct ArchivalProofNode
{
    uint32 index;
    Hash hash;
}

typedef ProofLevel ArchivalProofNode<>;

struct ArchivalProof
{
    uint32 epoch; // AST Subtree for this proof

    union switch (ArchivalProofType t)
    {
    case EXISTENCE:
        // Entries being proved
        ArchivalBucketEntry entries<>;

        // Vector of vectors, where proofLevels[level]
        // contains all HashNodes that correspond with that level
        ProofLevel proofLevels<>;

    case NONEXISTENCE:
        LedgerKey keysToProve<>;

        // Bounds for each key being proved, where bound[n]
        // corresponds to keysToProve[n]
        ArchivalBucketEntry lowBoundEntries<>;
        ArchivalBucketEntry highBoundEntries<>;

        // Vector of vectors, where proofLevels[level]
        // contains all HashNodes that correspond with that level
        ProofLevel proofLevels<>;
    } type;
};

enum OperationType
{
    ...
    INVOKE_HOST_FUNCTION_V2 = 27
};

struct Operation
{
    // sourceAccount is the account used to run the operation
    // if not set, the runtime defaults to "sourceAccount" specified at
    // the transaction level
    MuxedAccount* sourceAccount;

    union switch (OperationType type)
    {
    ...
    case INVOKE_HOST_FUNCTION_V2:
        InvokeHostFunctionOpV2 invokeHostFunctionOpV2;
    }
    body;
};

struct RestoreFootprintOp
{
    union switch (int v)
    {
    case 0:
        void;
    case 1:
        ArchivalProof proofs<>;
    }
    ext;
};

// Unfortunately this is necessary as InvokeHostFunctionOp does not have
// an extension point
struct InvokeHostFunctionOpV2
{
    // Host function to invoke.
    HostFunction hostFunction;
    // Per-address authorizations for this host function.
    SorobanAuthorizationEntry auth<>;
    ArchivalProof proofs<>; // must be proofs of nonexistence only

    ExtensionPoint ext;
};

struct LedgerCloseMetaV2
{
    ...
    // same as LedgerCloseMetaV1 up to here

    uint32 currentArchivalEpoch;

    // The last epoch currently stored by validators
    // Any entry restored from an epoch older than this will
    // require a proof.
    uint32 lastArchivalEpochPersisted;
};


union LedgerCloseMeta switch (int v)
{
case 0:
    LedgerCloseMetaV0 v0;
case 1:
    LedgerCloseMetaV1 v1;
case 2:
    LedgerCloseMetaV2 v2;
};

enum LedgerEntryChangeType
{
    LEDGER_ENTRY_CREATED = 0, // entry was added to the ledger
    LEDGER_ENTRY_UPDATED = 1, // entry was modified in the ledger
    LEDGER_ENTRY_REMOVED = 2, // entry was removed from the ledger
    LEDGER_ENTRY_STATE = 3,   // value of the entry
    LEDGER_ENTRY_RESTORE = 4  // archived entry was restored in the ledger
};

union LedgerEntryChange switch (LedgerEntryChangeType type)
{
case LEDGER_ENTRY_CREATED:
    LedgerEntry created;
case LEDGER_ENTRY_UPDATED:
    LedgerEntry updated;
case LEDGER_ENTRY_REMOVED:
    LedgerKey removed;
case LEDGER_ENTRY_STATE:
    LedgerEntry state;
case LEDGER_ENTRY_RESTORE:
    LedgerEntry restored;
};
```

### Semantics

#### Archival State Tree (AST)

The Archive State Tree (AST) is a collection of immutable Merkle trees whose leaves
are archived entries and `PERSISTENT` entry keys explicitly deleted via transaction
execution. The AST is a collection of subtrees indexed `AST[0]`, `AST[1]`, …, `AST[N]`.
The AST index number is called the Archival Epoch. We define the current
Archival Epoch as N + 1, where N is the index of the most recently completed AST
subtree.

For some Archival Epoch `k`, `AST[k]` is as follows:

`AST[k]` is a sorted, balanced, binary Merkle tree. When `AST[k]` is initialized, it
contains two dummy boundary leaves of type `BOUNDARY`, one lower bound leaf and
one upper bound leaf (these boundary leafs are required for proofs-of-nonexistence).

In addition to these boundary leafs, `AST[k]` contains a leaf for

1. Every `PERSISTENT` entry evicted (but not restored) during archival epoch `k`. These entries
are stored in `AST[k]` as type `INITENTRY`.

2. Every `PERSISTENT` entry explicitly deleted via transaction execution during epoch `k`.
These keys are stored in `AST[k]` as type `DEADENTRY`.

Leaf nodes are sorted as follows:

```
cmp(lhs: ArchivalBucketEntry, rhs: ArchivalBucketEntry): // lhs < rhs
    if lhs.type == HASHENTRY and rhs.type == HASHENTRY:
        if lhs.level != rhs.level:
            return lhs.level < rhs.level

        return lhs.index < rhs.index

    else if rhs.type == HASHENTRY:
        return true

    else if lhs.type == HASHENTRY:
        return false

    else if lhs.type == BOUNDARY and rhs.type == BOUNDARY
        return lhs.isLowerBound and not rhs.isLowerBound

    else if lhs.type == BOUNDARY:
        return lhs.isLowerBound

    else if rhs.type == BOUNDARY:
        return not rhs.isLowerBound

    else:
        return LedgerKey(lhs) < LedgerKey(rhs) // pre-existing compare function
```

Internal nodes, including the root node, are all of type `HASHENTRY` and constructed as follows:

Let us define a node's `level`, where leaf nodes have `level == 0`. For each leaf node with
`index == i`, there is the `HASHENTRY`:

```
HASHENTRY {
    level: 1
    index: i
    hash: SHA(node[0][i])
}
```

Note that the entire leaf node of type `ArchivalBucketEntry` is hashed, not just the underlying
`LedgerEntry` or `LedgerKey`. We must hash the entire `ArchivalBucketEntry` in order to verify
the index of the entry being proved (a requirement for proofs of nonexistence).

All internal nodes with `level > 1` are constructed as follows:

```
Given ChildNode {
    level: n
    index: i
}

InternalNode = HASHENTRY {
    level: n + 1
    index: i / 2
    hash: if node[n][i + 1] exists:
                SHA(node[n][i] + node[n][i + 1])
          else:
                SHA(node[n][i])
}
```

#### Proof Structure

In order to protect against "Double Restore" attacks (see Security Concerns), it is necessary to prove that the version of
the `LedgerEntry` being restored is the newest version that exists in the entire AST. This is accomplished by proving that the
entry exists in a given subtree, and proving that the key does not exist in every newer subtree.

A restoration proof for archived entry `e` that was archived in epoch `k` is as follows:

```
generateArchivalProof(e: ArchivedBucketEntry, k: Epoch):
    let key = LedgerEntryKey(e)
    let epochOfArchival = k
    let completeProof = vector<ArchivalProof>{}

    let proofOfExistence = generateProofOfExistenceProof(e, AST[k])
    completeProof.push_back(proofOfExistence)

    for epoch in range(epochOfArchival, currentEpoch):
        if archivalFilterMiss(LedgerKey(e), epoch): // See "Optimizing Proofs of Nonexistence"
            let proofOfNonexistence = generateProofOfNonexistence(k, AST[epoch])
            completeProof.push_back(proofOfNonexistence)

    return completeProof
```

##### Proof of existence

A proof of existence that `e` exists in some subtree is a Merkle style proof of inclusion. This includes
the path from the root node of the tree down to the leaf node being proved and all sibling nodes along
the path, excluding the root node. Since the root node is maintained by the validator validating the proof,
it can be omitted. Additionally, if multiple entries are being proved, each `HASHENTRY` on the proof path
must only be included once.

```
// Merkle subtree is represented by the map [level][node_index]
generateProofOfExistence(subtree=AST[k], e: ArchivalBucketEntry, proof: optional<ArchivalProof>):

    // If proof is not null, we are adding a new entry onto a pre-existing proof
    if not proof:
        proof = ArchivalProof(type = EXISTENCE)
        proof.epoch = k

    proof.entries.push_back(e)

    let curIndex = e.index

    for level in range(1, treeDepth - 1): // Omit root node
        let curNode = subtree[level][curIndex]

        // Insert path node if not already in proof
        if curNode not in proof.pathLevel[level]:
            proof.pathLevel[level].push_back(curNode)

        // insert neighbor node if not already in proof
        let neighborIndex = curIndex % 2 == 0 ? curIndex + 1 : curIndex - 1
        let neighborNode = subtree[level][neighborIndex]
        if neighbor_node not in proof.pathLevel[level]:
            proof.pathLevel[level].push_back(neighborNode)

        // Update for next level
        curIndex /= 2

    return proof
```

Given a proof `p` and the root of the subtree `r`,
a validator can verify the proof by recomputing the neighbor hashes along
the path and checking the result against the saved root hash as follows:

```
verifyProofOfExistence(p: ArchivalProof, r: ArchivalHashEntry):

    for entry in proof.entries:
        let expectedHash = SHA(entry)
        let level = 1
        let index = entry.index

        while level < r.level:
            let proofHash = proof.getNode(level, index)
            if proofHash.hash != expectedHash:
                return INVALID

            let neighborIndex = cur_index % 2 == 0 ? cur_index + 1 : cur_index - 1
            let neighborNode = proof.getNode(level, neighborIndex)

            expectedHash = SHA(expectedHash + neighborNode)
            level += 1
            index /= 2

        If expectedHash == r.hash:
            return VALID
        else:
            return INVALID

```

##### Proof of nonexistence

A proof of nonexistence demonstrates that for a given `LedgerKey k`, no such
key exists in the given subtree. A proof of nonexistence for `k` in a given
subtree is a proof of existence for two keys `lowKey` and `highKey` such that:

1. `lowKey < k`
2. `highKey > k`
3. `lowKey` and `highKey` are direct neighbors in the subtree,
    i.e. `subtree[highKey].index - subtree[lowKey].index == 1`

The proof is generated as follows:

```
// Merkle subtree is represented by the map [level][node_index]
generateProofOfNonexistence(subtree=AST[n], k: LedgerKey, proof: optional<ArchivalProof>):

    // If proof is not null, we are adding a new entry onto a pre-existing proof
    if not proof:
        proof = ArchivalProof(type = NONEXISTENCE)
        proof.epoch = k

    proof.keysToProve.push_back(k)

    // Get leaf node with lower_bound key
    // Note: Because of dummy boundary entries, there is always
    // some lower bound key in subtree < k
    let lowBoundLeaf = lower_bound(subtree[0], k)
    proof.lowBoundEntries.push_back(lowerBoundLeaf)

    // Note: Because of dummy boundary entries, there is always
    // some upper bound key in subtree > k
    let highBoundLeaf = subtree[0][lowBoundLeaf.index + 1]
    proof.highBoundEntries.push_back(highBoundLeaf)

    // Note: generateNonexistenceSubProof is functionally identical to generateProofOfExistence
    generateNonexistenceSubProof(subtree, lowerBoundLeaf, proof)
    generateNonexistenceSubProof(subtree, upperBoundLeaf, proof)
```

To verify a proof of nonexistence, validators verify that the
lowerBound and upperBound proofs of existence are valid, then
must check that the two entries provided are direct neighbors as follows:

```
verifyProofOfNonexistence(p: ArchivalProof, r: ArchivalHashEntry):

    for i in range(0, len(p.keysToProve)):
        let k = p.keysToProve[i]
        let lowBound = p.lowBoundEntries[i]
        let highBound = p.highBoundEntries[i]

        // Check that bounds are correct
        if lowerBound >= highBound:
            return INVALID

        if lowerBound.index != highBound.index + 1:
            return INVALID

        if lowBound >= k or highBound <= k:
            return INVALID

        // Note: verifyNonexistenceSubProof is functionally equivalent to verifyProofOfExistence
        if not verifyNonexistenceSubProof(lowBound, r, p)
                or not verifyNonexistenceSubProof(highBound, r, p):
            return INVALID

    return VALID
```

#### Generating the AST

Each validator maintains the "Live State BucketList" (currently called the BucketList). This stores all live ledger state,
including entries that are archived, but have not yet been evicted. Additionally, validators will maintain the "Hot Archive",
an additional BucketList containing recently archived entries. Validators will maintain an "Archival Snapshot Queue."
This queue contains all Hot Archives that have become full and are being converted into AST Merkle subtrees. Finally,
each validator will maintain the Merkle root of all AST subtrees, as well as an Archival Filter for each subtree. These roots
and filters are stored as `LedgerEntry` of type `BLOB` in the Live State BucketList.

#### Live State BucketList

The Live State BucketList most closely resembles the current BucketList. It will contain all “live” state of the ledger, including
Stellar classic entries, live Soroban entries, network config settings, etc. Additionally, it will include all AST information that is
permanent and must always be persisted by validators (Merkle roots and Archival Filters). This is a persistent store that is never deleted.
The Live State BucketList is published to the History Archive on every checkpoint ledger in the "BucketList state" section.

#### Hot Archive

The Hot Archive is a BucketList that stores recently evicted and deleted `PERSISTENT` entries. It contains `BucketEntry` type entries.
It is maintained as follows:

1. Whenever an entry is evicted, the entry is deleted from the Live State BucketList and added to the Hot Archive as an `INITENTRY`. When an
entry is evicted, the corresponding `TTLEntry` is deleted and not stored in the Hot Archive.
2. Whenever a `PERSISTENT` entry is deleted as part of transaction execution (not deleted via eviction event), the key is stored in the Hot
Archive as a `DEADENTRY`
3. If an archived entry is restored and the entry currently exists in the Hot Archive, the `INITENTRY` previously stored in the Hot Archive
is overwritten by a `RESTORED` entry.

The current Hot Archive is published to the History Archive in the "BucketList state" section on every checkpoint ledger.

Whenever the size of the Hot Archive exceeds `archivalSnapshotSize`, the Hot Archive becomes immutable and transitions to a
"Pending Archival Snapshot." At this point a new, empty Hot Archive is initialized and the current Archival Epoch is incremented.

The Pending Archival Snapshot are published to the History Archive in the "BucketList state" section on every checkpoint ledger.
Note that no Bucket files will actually be published, as the Pending Archival Snapshot is immutable.

Before a Hot Archive can be added to the Archival Snapshot Queue, it must be converted into an Archival Snapshot.

#### Archival Snapshot

An Archival Snapshot contains entries of type `ArchivalBucketEntry`. Whenever a Hot Archive enters the `Pending Archival Snapshot` state,
it is converted into an Archival Snapshot over many ledgers in a background thread. The conversion process is as follows:

1. All entries in the `Pending Archival Snapshot` will be condensed into a single, bottom level `Bucket`.
2. The Archival Snapshot will be initialized with a lower and upper bound value of type `BOUNDARY`.
3. All entries of type `RESTORED` will be dropped.
4. Entries of type `INITENTRY` will be converted to entries of type `LIVELEAF`.
5. Entries of type `DEADENTRY` will be converted to entries of type `DEADLEAF`.

Note that the initial Archival Snapshot has no entries of type `HASHENTRY`.

When the Hot Archive enters the `Pending Archival Snapshot` state, it is added into the Merkle Production Queue `numLedgersToInitSnapshot`
later. This is the amount of time a validator has to generate the Archival Snapshot via a background thread. When an Archival Snapshot
enters the Merkle Production Queue, it is published to the History Archive in the `archivalsnapshot` section. This is the only point at
which an Archival Snapshot will be published to the `archivalsnapshot` section of history.

The Archival Filter for the given snapshot is also generated during this time. Whenever the Archival Snapshot is added to the Merkle
Production Queue, the corresponding Archival Filter is committed to the Live State BucketList.

#### Merkle Production Queue

The Merkle Production Queue contains a vector of Archival Snapshots whose root Merkle hash has not yet
been generated. On each ledger, the front of the queue will gradually have more hashes computed according
to the `maxEntriesToHash` and `maxBytesToHash` limits. Once a root hash has been generated, the root hash
will be committed to the Live State BucketList and the queued Archive Snapshot will be dropped. Every
Archival Snapshot in this queue is published to History in the "BucketList state" section every checkpoint ledger.

#### Changes to History Archives

The `HistoryArchiveState` will be extended as follows:

```
version: number identifying the file format version
server: an optional debugging string identifying the software that wrote the file
networkPassphrase: an optional string identifying the networkPassphrase
currentLedger: a number denoting the ledger this file describes the state of
currentBuckets: an array containing an encoding of the live state bucket list for this ledger

hotArchiveBuckets: an array containing an encoding of the hot archive bucket list for this ledger

pendingSnapshotBuckets: an array containing an encoding of the pending archival snapshot bucket list for this ledger

merkleProductionQueueBuckets: an array of arrays containing an encoding of archival snapshot bucket lists for this ledger
```

Bucket files for the `hotArchiveBuckets`, `pendingSnapshotBuckets` and `merkleProductionQueueBuckets` will be stored
just as Bucket files are currently stored.

In addition to these changes, a new `archivalsnapshot` category will be added. This directory will contain
all Archival Snapshot, categorized by Archival Epoch, in the following format:

Each Archival Epoch will have a directory in the form `archivalsnapshot/ww/xx/yy/` where
`0xwwxxyyzz` is the 32bit ledger sequence number for the Archival Epoch at which the file was written,
expressed as an 8 hex digit, lower-case ASCII string. This directory will store a single Bucket file of the
form `archivalsnapshot/ww/xx/yy/archivalsnapshot-wwxxyyzz.xdr.gz`. While this file does not contain a complete
Merkle tree, it contains all necessary information to generate `AST[0xwwxxyyzz]`.

#### Changes to LedgerHeader

While the XDR structure of `LedgerHeader` remains unchanged, `bucketListHash` will be changed to the following:

```
header.bucketListHash = liveStateBucketListHash
                            + hotArchiveHash
                            + pendingArchiveHash // if exists
                            + merkleQueue[0].hash // if exists
                            + ... + merkleQueue[n].hash
```

#### InvokeHostFunctionOpV2

`InvocationHostFunctionOp` will be deprecated, and all host function invocations will be of type `InvokeHostFunctionOpV2`.

Prior to entering the VM, there will be an "archival phase" performed while loading entries from disk. The archival phase is
responsible for:

1. Validating proofs of nonexistence for newly created entries, and
2. Enforcing archival policies, i.e. failing the TX if the footprint contains an archived entry that has not been restored.

The archival phase works as follows:

```
for key in footprint.readonly:
    if key does not exist in live BucketList:
        failTx()

for key in footprint.readWrite:
    if key does not exist in live BucketList:
        // Assume key is being created for the first time

        // Incudes hot archive, pending archival snapshot, and every snapshot in Merkle Production Queue
        for bucketList in allBucketListsStoredOnDisk:
            if key exists in bucketlist:
                // Entry is archived
                failTx()

        // oldest epoch is epoch of oldest cold archive in queue, or hot archive if queue is empty
        for epoch in range(0, oldestEpochOnDisk):
            filter = getArchivalFilter(epoch)
            if key in filter:
                // Check for proof in case of filter false positive
                if validProof(key, epoch) not in invokeOp.proofs:
                    // Entry is archived, or filter miss occurred without overriding proof
                    failTx()
```

Following this archival phase, `InvokeHostFunctionOpV2` will function identically to `InvokeHostFunctionOp`. There are no archival
specific fees for creating a new entry even if a filter miss occurs and a proof must be verified
(see No Archival Fees for Entry Creation).

#### RestoreFootprintOp

`RestoreFootprintOp` will now be required to provide proofs for any archived entry that
is not currently stored by the validators. The operation can restore a mix of entries that
are archived but currently stored by validators and those that are not stored by validators.

`RestoreFootprintOp` will now require `instruction` resources, metered based on the complexity
of verifying the given proofs of inclusion. There are no additional fees or resources
required for proofs of nonexistence.

#### Meta

Meta will contain the current Archival Epoch via `currentArchivalEpoch`.This will be useful
for downstream diagnostics such as Hubble. Meta will also contain the oldest Archival Snapshot
persisted on validators via `lastArchivalEpochPersisted`. This will be useful to RPC preflight,
as it is the cutoff point at which a proof will need to be generated for `RestoreFootprintOp`.

Whenever a `PERSISTENT` entry is evicted (i.e. removed from the Live State BucketList and added to the Hot Archive),
the full entry will be emitted via `evictedPersistentLedgerEntries`.

Whenever an entry is restored via `RestoreFootprintOp`, the `LedgerEntry` being restored and its
associated TTL will be emitted as a `LedgerEntryChange` of type `LEDGER_ENTRY_RESTORE`. Note that
entries being restored may not have been evicted such that entries currently in the Live State BucketList
can see `LEDGER_ENTRY_RESTORE` events.

#### archivalBucketListSizeDiscount

The BucketList size used in fee calculations is derived as follows:

```
discount = (archivalBucketListSizeDiscountNumerator / archivalBucketListSizeDiscountDenominator)
size = liveStateBucketList.size + discount * (hotArchive.size + pendingSnapshot.size +
                                                sum_i(merkleQueue[i].size))
```

See Calculating BucketList Size for Fees for more.

#### Default Values for Network Configs

```
// Target false positive rate for filter = 1 / epsilon
uint64 filterEpsilon = 1 billion;
```

Bits per element == `1.1 * (1.075 log_2 (filterEpsilon))` (assuming [4-wise binary fuse](https://arxiv.org/pdf/2201.01174)).

```
epsilon:

1 billion   | 35 bits per entry, 4.37 MB per million entries
            | 190 MB for all current ledger state
100 million | 31 bits per entry, 3.87 MB per million entries
            | 168 MB for all current ledger state
```

At an epsilon of 1 billion, the entire ledger state would only consume 190 MB. In the entire history of the network,
there is a 4.3% change a single miss would have ocurred (assuming all entry creations required proof of nonexistence).
This seems like a reasonable balance between storage and false positive chances.

```
// When calculating rent rates based on BucketList size, discount archival
// Bucket List sizes by numerator / denominator.
uint64 archivalBucketListSizeDiscountNumerator = 0; // Default to no archival fee back pressure
uint64 archivalBucketListSizeDiscountDenominator = 1;
```

Default to not including archival related BucketList is size calculation (see Calculating BucketList Size for Fee).

```
// Rate limit how fast cold snapshots are converted to Merkle trees
uint32 maxEntriesToHash = 1000;
uint32 maxBytesToHash = 10 mb;
```

These are very modest, arbitrary starting limits that can be increased later.

```
// Number of entries at which archival snapshot is considered full
uint32 archivalSnapshotSize = 100;
```

Long term, 100 entries per archival snapshot is significantly smaller than is optimal.
However, given that there is currently a small number of expired `PERSISTENT` Soroban entries
(approximately 50 at the time of this writing), a small initial snapshot size will
allow us to evict the first round of persistent entries as a sort of "test run" for early
adopters before increasing this to a larger, more reasonable value.

```
// Number of levels in Archival Snapshot BucketList
uint32 archivalSnapshotDepth = 4;
```

This value should be increased to scale with `archivalSnapshotSize`. Since there is
a very low initial `archivalSnapshotSize` value, this value should also begin very low.

```
// The number of ledger between which the Hot Archive becomes full
// and when the corresponding archival snapshot is initialized and
// added to Merkle Production Queue
uint32 numLedgersToInitSnapshot = 1000; // 1.5 hours
```

Currently, even the largest Bucket merge events take less than 10 minutes. This value
is very conservative in order to support many different SKUs and provide flexibility
to instances that may have burst limited storage mediums.

## Design Rationale

### Optimizing Proofs of Nonexistence

Proofs of existence cannot be optimized. However, proofs of nonexistence are very expensive and must be included
for both entry restoration, as well as initial entry creation. To maintain security but improve performance,
proofs of nonexistence will be optimized via binary fuse [filters](https://arxiv.org/abs/2201.01174).
A binary fuse filter is a probabilistic set that is
significantly smaller than deterministic sets, but has some false positives, i.e. if an entry does not exist in the set,
a binary fuse filter will occasionally return a false positive erroneously indicating that the element exists in the set
when it does not. However, if an element does exist in a set, the binary fuse filter will always deterministically return
the correct result such that false negatives are not possible.

In addition to maintaining the root hash for each AST subtree, validators will also need to maintain a binary fuse filter
for the set of keys contained in the subtree. While technically this means that validators will have to maintain unbounded
state, the amount of state is sufficiently small. Assuming a false positive rate of 1 out of a billion, a binary fuse filter
will require approximately 32.14 megabytes per million archived keys. Additionally, binary fuse filters are good candidates for
compression, further reducing persistent disk requirements.

If a proof of nonexistence is required for a given transaction but not included, validators will check the binary fuse filter
of the given subtree. If the filter indicates that the element does not exist in the subtree, no proof of exclusion for the given
subtree is required. Since binary fuse filters cannot produce a false negative (i.e. the filter indicates a key does not exist in
the subtree when it actually does) all security guarantees are upheld. In the worst case, a false positive is returned such that
the filter indicates the entry does exist in the subtree when it does not. In this case, a full proof of nonexistence must be
provided for the transaction to succeed.

### No Archival Fees for Entry Creation

While additional fees are charged for verifying proofs of existence, no fees are charged for validating proofs of nonexistence.
There are several potential ways to break smart contracts should we charge for proofs of nonexistence:

1. Meter instructions for each binary fuse filter lookup. As the age of the network increases and more Archival Snapshots
are produced, the number of filter lookups, and thus the instruction cost, increase linearly. This means that a transaction
that could fit within transaction instruction limits at a given epoch may not be valid in a future epoch. This is a significant
footgun, as developers wish to maximize transaction complexity according to the current limits.

2. Meter instructions for proof of nonexistence verification. Proofs of nonexistence should only be required very rarely,
with the default false positive rate set to 1 out of a billion. This means that it is highly unlikely contract developers
will account for the possibility of nonexistence proofs when writing contracts. A sufficiently complex contract may be close
to transaction instruction limits in the "happy path," where no proof is required. Should this contract attempt to create a
new key in the "unhappy path," the cost of proof verification will exceed transaction limits. This essentially bricks the
contract, making the otherwise valid transaction invalid due to a highly unlikely probabilistic event. Should developers
account for this case, they would be under utilizing available transaction limits for a highly unlikely edge case.

At a given epoch, every new entry creation will require the same amount of filter instructions (assuming no filter misses).
Several control mechanisms already exist to control entry creation price, such as write fees and minimum rent balances. Given
potential footguns and that control mechanisms on entry creation already exists, there is no need for a filter specific resource
charge.

As for filter misses, it seems highly unlikely they could be abused to DOS the network. While the instruction consumption
of proofs of exclusion is not metered, the transactions still have significant cost due to the increased transaction size
incurred by including a proof. Finally the actual compute cost of verifying proofs is not significant, as it requires no
disk lookups and only a small handful in-memory hash operations. The transaction size cost incurred by the attacker would
significantly outweigh the network cost of the non metered computation.

### Expectations for Downstream Systems

AST subtrees are deeply tied to BucketList structure. While `stellar-core` has an integrated high performance database
built on top of the BucketList, there does not yet exist a standalone BucketList database. For this reason, all queries
regarding archival state and proof production will be managed by `stellar-core`.

RPC instances preflighting transactions are expected to use the following `captive-core` endpoints when interacting with
archived state:

1. `getledgerentry` Given a set of keys, this endpoint will return the corresponding `LedgerEntry` along with archival
related information. Specifically, this endpoint will return whether the entry is archived, and if it is archived, whether
or not it requires a proof to be restored. Additionally, if a given key does not exist, this endpoint will return whether or
not the entry creation requires a proof of nonexistence. An example return payload would be as follows:

```
{
    // Entry exists in Live State BucketList
    {
    "key": Ledgerkey
    "entry": LedgerEntry
    "state": "live"
    },

    // Entry is archived, but no proof is required since the entry has been recently archived
    {
    "key": Ledgerkey
    "entry": LedgerEntry
    "state": "archived-noproof"
    "epoch": archivalEpoch
    },

    // Entry is archived and requires a proof
    {
    "key": Ledgerkey
    "entry": LedgerEntry
    "state": "archived-proof"
    "epoch": archivalEpoch
    },

    // Entry has not yet been created, does not require a proof
    {
    "key": Ledgerkey
    "state": "nonexistence-noproof"
    },
}
```

2. `generateproof` Given a set of keys, this endpoint will return an `ArchivalProof` sufficient to prove the entries. This
endpoint will provide both proofs of existence and nonexistence.

While historically downstream systems have not been advised to use captive-core endpoints for queries, recent parallelization
efforts have made this possible.

### History Archive

In order for validators to join the network efficiently, all BucketLists required for consensus are stored in the
History Archive State. While this does increase the size of the History Archives in the short term, there is no
strong reason that History Archive State files must be persisted permanently at 64 ledger resolution. In the future,
Tier 1 policy could be modified such that only resent History Archive State files need to be maintained at checkpoint
resolution to assist in node catchup, while older History Archive State files could be dropped or stored at a lower resolution.
Note that this does not impact the audibility of the network, as all transaction history will still be maintained.

In the long term, the only State Archival files that will need to be persisted permanently in the archive are Archive
Snapshot files. In order to keep this persisted data as small as possible, only the leaf nodes of these files are stored.
This provides all the necessary information to construct the full Merkle tree required to generate proofs, but is significantly
smaller. There is no security risk either, as after a full Merkle tree has been constructed from these files, the root can
be checked against the roots stored in the validator's current live state.

In addition to saving disk space, this strategy has the added advantage of reducing Archival Snapshot egress. Currently,
egress fees are the dominating cost of History Archives. Should a complete Merkle tree exist in the History Archive, it
would be easy to develop a lightweight application to generate a proof directly from the History Archive file, bypassing
RPC nodes. There have been previous issues with similar applications abusing public History Archives, where large files are
constantly downloaded and subsequently deleted repeatedly by such programs. By only storing leaf nodes, it is necessary for an
RPC instance to perform an initialization step before generating proofs. While this step will still be relatively fast and
cheap, it encourages proper data hygiene, protecting History Archive providers.

### Long Term RPC Sustainability

The AST has been designed such that an RPC node may store as many or as few Archival Snapshots as the operator desires.
For example, an RPC provider may only serve the last two years worth of Archival Snapshots, such that any archived entry
older than two years old cannot be restored by that node. Additionally, RPC providers may shard Archival Snapshots across
many different nodes and route requests accordingly.

While this is possible long term, the size of the Archival Snapshots will not be significant enough to warrant this
additional complexity. For the forseeable future, RPC providers should expect to maintain all Archival Snapshots. However,
this does not need to be the case in the future should Archival State grow significantly.

### Calculating BucketList Size for Fees

It is not immediately clear whether or not BucketLists other than the Live State BucketList should
count towards BucketList size used in fee calculations. The Live State BucketList is a leading
indicator of network state usage, while archival related BucketLists are lagging indicators. By counting
archival related BucketLists in fee calculation, the network is penalized more from validator
implementation details then penalized for actual network usage. Additionally, archival related
BucketLists will introduce significant fee volatility on the network, as entire BucketLists will
be dropped whenever a Merkle hash is generated. These reasons support only counting Live State BucketList
size towards fees.

However, the primary purpose behind size related fees is to apply back pressure when validator disk usage
is high. Archival related BucketLists are stored on disk and therefore contribute to the storage issue,
such that they should be included in fee calculations to provide appropriate back pressure.

Ideally, archival related BucketLists should not count towards fees to provide the best experience to
network users. The rate of Archival Snapshot production and root hash production should outpace that
of eviction and entry creation. Should this be the case, there is no need for additional size related fees.
However, should additional back pressure be necessary, the discount ratio may be used to scale pressure
accordingly. The discount ratio will default to 0 and be adjusted accordingly.

## Security Concerns

Many attack vectors have been considered, including the following:

### Double Nonce

Consider a one-time nonce created at a deterministic address (a common design practice for minting NFTs). The
legitimate owner of the nonce creates the entry in Archival Epoch N, and the nonce entry is subsequently archived
and evicted. Much later in epoch N + 5, and attacker attempts to recreate the nonce at the same address, but this time
with the attacker as owner. The validators do not currently store the original nonce entry, and have no way of knowing
the nonce exists and is owned by another entity. Because the validators believe this entry is being created for the first
time, the nonce is recreated and the attacker now has established control.

To defeat this, proofs of nonexistence are required when creating an entry for the first time. In the scenario above,
because the nonce exists in the archive, no valid proof of nonexistence for the key exists, and the attacker would not
be able to recreate the entry.

### Double Spend via Multiple Restores

Consider a balance entry with a non-zero balance that has been evicted and is no longer stored by validators. The attacker
restores the entry, spends the balance, then deletes the balance entry. However, even though the entry has been deleted
in the Live State, the non-zero version of the balance entry still exists in the archive. The attacker then restores the non-zero
balance entry again. The entry is added back to the Live State with a non-zero balance, resulting in a double spend.

The defense against this attack is to write deletion events into the Archival Snapshot. When restoring an entry, the proof
must verify that the key exists in the given Archival Snapshot and does not exist in any newer epoch. Because deletion events
are written to Archival Snapshots, the attacker will not be able to prove that no newer version of the balance key exists
(as the deletion key is newer than the non-zero balance entry) defeating the attack.

### Issues with Requiring Authorization for Restoration

Consider a design where authorization was required for restoration such that only the entry owner could restore an archived
entry. Consider a loan contract, where an attacker has put up some collateral for a loan. The collateral balance entries are
archived, and the attacker defaults on the loan. When the loan contract attempts to collect the collateral, the entries are
archived, and the contract cannot restore the entries to collect collateral.

To defeat this attack, authorization is not required for restoration. There is no significant security concern with allowing
arbitrary restoration. Even though any entity can restore an entry, authorization is still required to modify the entry
once restored.

## Test Cases

Specific test cases are still TBD. Formal verification of proof generation and validation may be useful.

## Implementation

TBD
